{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-08T17:51:54.142189Z",
     "start_time": "2025-01-08T17:51:52.595547Z"
    }
   },
   "source": [
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "import jax.numpy as np\n",
    "from jax import random\n",
    "from jax.scipy.linalg import block_diag\n",
    "\n",
    "from s5.dataloading import Datasets\n",
    "from s5.seq_model import BatchClassificationModel, RetrievalModel\n",
    "from s5.ssm import init_S5SSM\n",
    "from s5.ssm_init import make_DPLR_HiPPO\n",
    "from s5.train_helpers import create_train_state, reduce_lr_on_plateau, \\\n",
    "    linear_warmup, cosine_annealing, constant_lr, train_epoch, validate"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T18:45:00.871067Z",
     "start_time": "2025-01-08T18:45:00.866476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "\n",
    "# Create the args object\n",
    "args = Args(\n",
    "    ssm_size=256,\n",
    "    jax_seed=0,\n",
    "    blocks=4,\n",
    "    d_model=128,\n",
    "    n_layers=6,\n",
    "    n_classes=2,\n",
    "    C_init='complex_normal',\n",
    "    discretization='zoh',\n",
    "    dt_min=0.001,\n",
    "    dt_max=0.1,\n",
    "    conj_sym=True,\n",
    "    clip_eigs=True,\n",
    "    bidirectional=False,\n",
    "    activation_fn='gelu',\n",
    "    dropout=0.0,\n",
    "    prenorm=True,\n",
    "    batchnorm=True,\n",
    "    bn_momentum=0.9,\n",
    "    dir_name='./cache_dir',\n",
    "    bsz=64,\n",
    "    ssm_size_base=256,\n",
    "    p_dropout=0.0,\n",
    "    mode='pool',\n",
    "    lr_factor=1,\n",
    "    ssm_lr_base=1e-3,\n",
    "    weight_decay=0.05,\n",
    "    opt_config='standard',\n",
    "    dt_global=False,\n",
    "    dataset='mnist-classification',\n",
    "    epochs=1,\n",
    "    warmup_end=1,\n",
    "    lr_min=0,\n",
    ")"
   ],
   "id": "61841906f04d8383",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T18:45:02.523067Z",
     "start_time": "2025-01-08T18:45:02.506360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(args):\n",
    "    \"\"\"\n",
    "    Main function to train over a certain number of epochs\n",
    "    \"\"\"\n",
    "\n",
    "    best_test_loss = 100000000\n",
    "    best_test_acc = -10000.0\n",
    "\n",
    "    ssm_size = args.ssm_size_base\n",
    "    ssm_lr = args.ssm_lr_base\n",
    "\n",
    "    # determine the size of initial blocks\n",
    "    block_size = int(ssm_size / args.blocks)\n",
    "\n",
    "    # Set global learning rate lr (e.g. encoders, etc.) as function of ssm_lr\n",
    "    lr = args.lr_factor * ssm_lr\n",
    "\n",
    "    # Set randomness...\n",
    "    print(\"[*] Setting Randomness...\")\n",
    "    key = random.PRNGKey(args.jax_seed)\n",
    "    init_rng, train_rng = random.split(key, num=2)\n",
    "\n",
    "    # Get dataset creation function\n",
    "    create_dataset_fn = Datasets[args.dataset]\n",
    "\n",
    "    # Dataset dependent logic\n",
    "    if args.dataset in [\"imdb-classification\", \"listops-classification\", \"aan-classification\"]:\n",
    "        padded = True\n",
    "        if args.dataset in [\"aan-classification\"]:\n",
    "            # Use retreival model for document matching\n",
    "            retrieval = True\n",
    "            print(\"Using retrieval model for document matching\")\n",
    "        else:\n",
    "            retrieval = False\n",
    "\n",
    "    else:\n",
    "        padded = False\n",
    "        retrieval = False\n",
    "\n",
    "    # For speech dataset\n",
    "    if args.dataset in [\"speech35-classification\"]:\n",
    "        speech = True\n",
    "        print(\"Will evaluate on both resolutions for speech task\")\n",
    "    else:\n",
    "        speech = False\n",
    "\n",
    "    # Create dataset...\n",
    "    init_rng, key = random.split(init_rng, num=2)\n",
    "    trainloader, valloader, testloader, aux_dataloaders, n_classes, seq_len, in_dim, train_size = \\\n",
    "        create_dataset_fn(args.dir_name, seed=args.jax_seed, bsz=args.bsz)\n",
    "\n",
    "    print(f\"[*] Starting S5 Training on `{args.dataset}` =>> Initializing...\")\n",
    "\n",
    "    # Initialize state matrix A using approximation to HiPPO-LegS matrix\n",
    "    Lambda, _, B, V, B_orig = make_DPLR_HiPPO(block_size)\n",
    "\n",
    "    if args.conj_sym:\n",
    "        block_size = block_size // 2\n",
    "        ssm_size = ssm_size // 2\n",
    "\n",
    "    Lambda = Lambda[:block_size]\n",
    "    V = V[:, :block_size]\n",
    "    Vc = V.conj().T\n",
    "\n",
    "    # If initializing state matrix A as block-diagonal, put HiPPO approximation\n",
    "    # on each block\n",
    "    Lambda = (Lambda * np.ones((args.blocks, block_size))).ravel()\n",
    "    V = block_diag(*([V] * args.blocks))\n",
    "    Vinv = block_diag(*([Vc] * args.blocks))\n",
    "\n",
    "    print(\"Lambda.shape={}\".format(Lambda.shape))\n",
    "    print(\"V.shape={}\".format(V.shape))\n",
    "    print(\"Vinv.shape={}\".format(Vinv.shape))\n",
    "\n",
    "    ssm_init_fn = init_S5SSM(H=args.d_model,\n",
    "                             P=ssm_size,\n",
    "                             Lambda_re_init=Lambda.real,\n",
    "                             Lambda_im_init=Lambda.imag,\n",
    "                             V=V,\n",
    "                             Vinv=Vinv,\n",
    "                             C_init=args.C_init,\n",
    "                             discretization=args.discretization,\n",
    "                             dt_min=args.dt_min,\n",
    "                             dt_max=args.dt_max,\n",
    "                             conj_sym=args.conj_sym,\n",
    "                             clip_eigs=args.clip_eigs,\n",
    "                             bidirectional=args.bidirectional)\n",
    "\n",
    "    if retrieval:\n",
    "        # Use retrieval head for AAN task\n",
    "        print(\"Using Retrieval head for {} task\".format(args.dataset))\n",
    "        model_cls = partial(\n",
    "            RetrievalModel,\n",
    "            ssm=ssm_init_fn,\n",
    "            d_output=n_classes,\n",
    "            d_model=args.d_model,\n",
    "            n_layers=args.n_layers,\n",
    "            padded=padded,\n",
    "            activation=args.activation_fn,\n",
    "            dropout=args.p_dropout,\n",
    "            prenorm=args.prenorm,\n",
    "            batchnorm=args.batchnorm,\n",
    "            bn_momentum=args.bn_momentum,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        model_cls = partial(\n",
    "            BatchClassificationModel,\n",
    "            ssm=ssm_init_fn,\n",
    "            d_output=n_classes,\n",
    "            d_model=args.d_model,\n",
    "            n_layers=args.n_layers,\n",
    "            padded=padded,\n",
    "            activation=args.activation_fn,\n",
    "            dropout=args.p_dropout,\n",
    "            mode=args.mode,\n",
    "            prenorm=args.prenorm,\n",
    "            batchnorm=args.batchnorm,\n",
    "            bn_momentum=args.bn_momentum,\n",
    "        )\n",
    "\n",
    "    # initialize training state\n",
    "    state = create_train_state(model_cls,\n",
    "                               init_rng,\n",
    "                               padded,\n",
    "                               retrieval,\n",
    "                               in_dim=in_dim,\n",
    "                               bsz=args.bsz,\n",
    "                               seq_len=seq_len,\n",
    "                               weight_decay=args.weight_decay,\n",
    "                               batchnorm=args.batchnorm,\n",
    "                               opt_config=args.opt_config,\n",
    "                               ssm_lr=ssm_lr,\n",
    "                               lr=lr,\n",
    "                               dt_global=args.dt_global)\n",
    "\n",
    "    # Training Loop over epochs\n",
    "    best_loss, best_acc, best_epoch = 100000000, -100000000.0, 0  # This best loss is val_loss\n",
    "    count, best_val_loss = 0, 100000000  # This line is for early stopping purposes\n",
    "    lr_count, opt_acc = 0, -100000000.0  # This line is for learning rate decay\n",
    "    step = 0  # for per step learning rate decay\n",
    "    steps_per_epoch = int(train_size / args.bsz)\n",
    "    for epoch in range(args.epochs):\n",
    "        print(f\"[*] Starting Training Epoch {epoch + 1}...\")\n",
    "\n",
    "        if epoch < args.warmup_end:\n",
    "            print(\"using linear warmup for epoch {}\".format(epoch + 1))\n",
    "            decay_function = linear_warmup\n",
    "            end_step = steps_per_epoch * args.warmup_end\n",
    "\n",
    "        elif args.cosine_anneal:\n",
    "            print(\"using cosine annealing for epoch {}\".format(epoch + 1))\n",
    "            decay_function = cosine_annealing\n",
    "            # for per step learning rate decay\n",
    "            end_step = steps_per_epoch * args.epochs - (steps_per_epoch * args.warmup_end)\n",
    "        else:\n",
    "            print(\"using constant lr for epoch {}\".format(epoch + 1))\n",
    "            decay_function = constant_lr\n",
    "            end_step = None\n",
    "\n",
    "        # TODO: Switch to letting Optax handle this.\n",
    "        #  Passing this around to manually handle per step learning rate decay.\n",
    "        lr_params = (decay_function, ssm_lr, lr, step, end_step, args.opt_config, args.lr_min)\n",
    "\n",
    "        train_rng, skey = random.split(train_rng)\n",
    "        state, train_loss, step = train_epoch(state,\n",
    "                                              skey,\n",
    "                                              model_cls,\n",
    "                                              trainloader,\n",
    "                                              seq_len,\n",
    "                                              in_dim,\n",
    "                                              args.batchnorm,\n",
    "                                              lr_params)\n",
    "\n",
    "        if valloader is not None:\n",
    "            print(f\"[*] Running Epoch {epoch + 1} Validation...\")\n",
    "            val_loss, val_acc = validate(state,\n",
    "                                         model_cls,\n",
    "                                         valloader,\n",
    "                                         seq_len,\n",
    "                                         in_dim,\n",
    "                                         args.batchnorm)\n",
    "\n",
    "            print(f\"[*] Running Epoch {epoch + 1} Test...\")\n",
    "            test_loss, test_acc = validate(state,\n",
    "                                           model_cls,\n",
    "                                           testloader,\n",
    "                                           seq_len,\n",
    "                                           in_dim,\n",
    "                                           args.batchnorm)\n",
    "\n",
    "            print(f\"\\n=>> Epoch {epoch + 1} Metrics ===\")\n",
    "            print(\n",
    "                f\"\\tTrain Loss: {train_loss:.5f} -- Val Loss: {val_loss:.5f} --Test Loss: {test_loss:.5f} --\"\n",
    "                f\" Val Accuracy: {val_acc:.4f}\"\n",
    "                f\" Test Accuracy: {test_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # else use test set as validation set (e.g. IMDB)\n",
    "            print(f\"[*] Running Epoch {epoch + 1} Test...\")\n",
    "            val_loss, val_acc = validate(state,\n",
    "                                         model_cls,\n",
    "                                         testloader,\n",
    "                                         seq_len,\n",
    "                                         in_dim,\n",
    "                                         args.batchnorm)\n",
    "\n",
    "            print(f\"\\n=>> Epoch {epoch + 1} Metrics ===\")\n",
    "            print(\n",
    "                f\"\\tTrain Loss: {train_loss:.5f}  --Test Loss: {val_loss:.5f} --\"\n",
    "                f\" Test Accuracy: {val_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "        # For early stopping purposes\n",
    "        if val_loss < best_val_loss:\n",
    "            count = 0\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            count += 1\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            # Increment counters etc.\n",
    "            count = 0\n",
    "            best_loss, best_acc, best_epoch = val_loss, val_acc, epoch\n",
    "            if valloader is not None:\n",
    "                best_test_loss, best_test_acc = test_loss, test_acc\n",
    "            else:\n",
    "                best_test_loss, best_test_acc = best_loss, best_acc\n",
    "\n",
    "        # For learning rate decay purposes:\n",
    "        input = lr, ssm_lr, lr_count, val_acc, opt_acc\n",
    "        lr, ssm_lr, lr_count, opt_acc = reduce_lr_on_plateau(input, factor=1.0,\n",
    "                                                             patience=1000000, lr_min=0)\n",
    "\n",
    "        # Print best accuracy & loss so far...\n",
    "        print(\n",
    "            f\"\\tBest Val Loss: {best_loss:.5f} -- Best Val Accuracy:\"\n",
    "            f\" {best_acc:.4f} at Epoch {best_epoch + 1}\\n\"\n",
    "            f\"\\tBest Test Loss: {best_test_loss:.5f} -- Best Test Accuracy:\"\n",
    "            f\" {best_test_acc:.4f} at Epoch {best_epoch + 1}\\n\"\n",
    "        )\n",
    "\n",
    "        # if count > args.early_stop_patience:\n",
    "        #     break\n",
    "\n",
    "    filename = f\"./quick_test.pkl\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(state.params, f)\n",
    "    print(f\"Model weights saved to {filename}\")"
   ],
   "id": "d92ef6269c555f0e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T19:23:27.559115Z",
     "start_time": "2025-01-08T18:45:05.466349Z"
    }
   },
   "cell_type": "code",
   "source": "train(args)",
   "id": "2ee0fd9cb98b24e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Setting Randomness...\n",
      "[*] Generating MNIST Classification Dataset\n",
      "[*] Starting S5 Training on `mnist-classification` =>> Initializing...\n",
      "Lambda.shape=(128,)\n",
      "V.shape=(256, 128)\n",
      "Vinv.shape=(128, 256)\n",
      "configuring standard optimization setup\n",
      "[*] Trainable Parameters: 399370\n",
      "[*] Starting Training Epoch 1...\n",
      "using linear warmup for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 843/843 [35:54<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Running Epoch 1 Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:59<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Running Epoch 1 Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:26<00:00,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=>> Epoch 1 Metrics ===\n",
      "\tTrain Loss: 0.72254 -- Val Loss: 0.16392 --Test Loss: 0.15577 -- Val Accuracy: 0.9465 Test Accuracy: 0.9507\n",
      "\tBest Val Loss: 0.16392 -- Best Val Accuracy: 0.9465 at Epoch 1\n",
      "\tBest Test Loss: 0.15577 -- Best Test Accuracy: 0.9507 at Epoch 1\n",
      "\n",
      "Model weights saved to ./quick_test.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e90b7241364f82bc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

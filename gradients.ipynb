{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-16T18:57:23.429340Z",
     "start_time": "2025-01-16T18:57:22.094257Z"
    }
   },
   "source": [
    "import pickle\n",
    "from functools import partial\n",
    "import itertools\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from jax import random\n",
    "from jax.scipy.linalg import block_diag\n",
    "from tqdm import tqdm\n",
    "\n",
    "from s5.dataloading import Datasets\n",
    "from s5.seq_model import BatchClassificationModel, RetrievalModel\n",
    "from s5.ssm import init_S5SSM\n",
    "from s5.ssm_init import make_DPLR_HiPPO\n",
    "from s5.train_helpers import (create_train_state, reduce_lr_on_plateau, \\\n",
    "    linear_warmup, cosine_annealing, constant_lr, validate, prep_batch, train_step, update_learning_rate_per_step, cross_entropy_loss)\n",
    "from s5.gradients import gradient_monitoring_hook"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T18:57:23.935385Z",
     "start_time": "2025-01-16T18:57:23.930700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "\n",
    "# Create the args object\n",
    "args = Args(\n",
    "    ssm_size=128,\n",
    "    jax_seed=0,\n",
    "    blocks=1,\n",
    "    d_model=64,\n",
    "    n_layers=1,\n",
    "    n_classes=2,\n",
    "    C_init='complex_normal',\n",
    "    discretization='zoh',\n",
    "    dt_min=0.001,\n",
    "    dt_max=0.1,\n",
    "    conj_sym=True,\n",
    "    clip_eigs=True,\n",
    "    bidirectional=False,\n",
    "    activation_fn='gelu',\n",
    "    dropout=0.0,\n",
    "    prenorm=True,\n",
    "    batchnorm=False,\n",
    "    bn_momentum=0.9,\n",
    "    dir_name='./cache_dir',\n",
    "    bsz=16,\n",
    "    ssm_size_base=256,\n",
    "    p_dropout=0.0,\n",
    "    mode='pool',\n",
    "    lr_factor=1,\n",
    "    ssm_lr_base=1e-3,\n",
    "    weight_decay=0.05,\n",
    "    opt_config='standard',\n",
    "    dt_global=False,\n",
    "    dataset='mnist-classification',\n",
    "    epochs=1,\n",
    "    warmup_end=1,\n",
    "    lr_min=0,\n",
    ")"
   ],
   "id": "f00088433985b75b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "let's try to solve a simpler problem, just give me the hidden state at layer i at time t for a single batch.",
   "id": "be4b3b8e0efd111e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T23:48:54.398877Z",
     "start_time": "2025-01-16T23:48:54.363734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hidden_gradients(state,\n",
    "               rng,\n",
    "               batch_inputs,\n",
    "               batch_integration_timesteps,\n",
    "               model,\n",
    "               batchnorm,\n",
    "               ):\n",
    "    \"\"\"Computes gradients of hidden states with respect to parameters and inputs\"\"\"\n",
    "    \n",
    "    def hidden_fn(inputs, params):\n",
    "        \"\"\"Returns only the hidden states from the model\"\"\"\n",
    "        if batchnorm:\n",
    "            _, hiddens = model.apply(\n",
    "                {\"params\": params, \"batch_stats\": state.batch_stats},\n",
    "                inputs, batch_integration_timesteps,\n",
    "                rngs={\"dropout\": rng},\n",
    "                mutable=False  # We don't need to track batch stats anymore\n",
    "            )\n",
    "            print(hiddens)\n",
    "        else:\n",
    "            _, hiddens = model.apply(\n",
    "                {\"params\": params},\n",
    "                inputs, batch_integration_timesteps,\n",
    "                rngs={\"dropout\": rng},\n",
    "                mutable=False,\n",
    "            )\n",
    "            # print(hiddens.shape)\n",
    "            # print(np.abs(hiddens).shape)\n",
    "        return np.abs(hiddens)\n",
    "    def hidden_at_time(inputs, params, time, dimension):\n",
    "        \"\"\"Returns the hidden state at a specific time and dimension\"\"\"\n",
    "        hiddens = hidden_fn(inputs, params) # shape (bsz, time, hidden_dim)\n",
    "        # print(np.mean(hiddens[:, time, dimension]).shape)\n",
    "        return np.mean(hiddens[:, time, dimension])\n",
    "    print(batch_inputs.shape)\n",
    "    gradient = jax.grad(hidden_at_time, argnums=0)(batch_inputs, state.params, 784//2, 0)\n",
    "    print(f\"Gradient: {gradient.squeeze().shape}\")\n",
    "    print(gradient.squeeze())\n",
    "    # print(batch_inputs.shape)\n",
    "    # # Compute gradients with respect to parameters\n",
    "    # param_grads = jax.grad(lambda p: hidden_fn(p, batch_inputs).sum())(state.params)\n",
    "    # \n",
    "    # # Compute gradients with respect to inputs\n",
    "    # input_grads = jax.grad(lambda x: hidden_fn(state.params, x).sum())(batch_inputs)\n",
    "    # \n",
    "    # # Get the hidden states\n",
    "    # hiddens = hidden_fn(state.params, batch_inputs)\n",
    "    # \n",
    "    # return hiddens, param_grads, input_grads"
   ],
   "id": "b62cb11a8e676e0",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T23:48:54.956066Z",
     "start_time": "2025-01-16T23:48:54.926681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@partial(jax.jit, static_argnums=(5, 6))\n",
    "def hidden_gradient_step(state,\n",
    "               rng,\n",
    "               batch_inputs,\n",
    "               batch_labels,\n",
    "               batch_integration_timesteps,\n",
    "               model,\n",
    "               batchnorm,\n",
    "               ):\n",
    "    \"\"\"Performs a single training step given a batch of data\"\"\"\n",
    "    def loss_fn(params):\n",
    "\n",
    "        if batchnorm:\n",
    "            (logits, hiddens), mod_vars = model.apply(\n",
    "                {\"params\": params, \"batch_stats\": state.batch_stats},\n",
    "                batch_inputs, batch_integration_timesteps,\n",
    "                rngs={\"dropout\": rng},\n",
    "                mutable=[\"intermediates\", \"batch_stats\"],\n",
    "                capture_intermediates=True\n",
    "            )\n",
    "        else:\n",
    "            (logits, hiddens), mod_vars = model.apply(\n",
    "                {\"params\": params},\n",
    "                batch_inputs, batch_integration_timesteps,\n",
    "                rngs={\"dropout\": rng},\n",
    "                mutable=[\"intermediates\"],\n",
    "            )\n",
    "            print(\"got here\")\n",
    "            print(mod_vars)\n",
    "\n",
    "        loss = np.mean(cross_entropy_loss(logits, batch_labels))\n",
    "\n",
    "        return loss, (mod_vars, logits)\n",
    "\n",
    "    (loss, (mod_vars, logits)), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    if batchnorm:\n",
    "        state = state.apply_gradients(grads=grads, batch_stats=mod_vars[\"batch_stats\"])\n",
    "    else:\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "def train_epoch(state, rng, model, trainloader, seq_len, in_dim, batchnorm, lr_params):\n",
    "    \"\"\"\n",
    "    Training function for an epoch that loops over batches.\n",
    "    \"\"\"\n",
    "    # Store Metrics\n",
    "    model = model(training=True)\n",
    "    batch_losses = []\n",
    "\n",
    "    decay_function, ssm_lr, lr, step, end_step, opt_config, lr_min = lr_params\n",
    "\n",
    "    for batch_idx, batch in enumerate(itertools.islice(tqdm(trainloader), 10)):\n",
    "        inputs, labels, integration_times = prep_batch(batch, seq_len, in_dim)\n",
    "        rng, drop_rng = jax.random.split(rng)\n",
    "        hiddens, param_grads, input_grads = hidden_gradients(\n",
    "            state=state,\n",
    "            rng=rng,\n",
    "            batch_inputs=inputs,\n",
    "            batch_integration_timesteps=integration_times,\n",
    "            model=model,\n",
    "            batchnorm=batchnorm\n",
    "        )\n",
    "        # print(\"THIS SEEMS TO WORK SOMEHOW\")\n",
    "        # state, loss = hidden_gradient_step(\n",
    "        #     state,\n",
    "        #     drop_rng,\n",
    "        #     inputs,\n",
    "        #     labels,\n",
    "        #     integration_times,\n",
    "        #     model,\n",
    "        #     batchnorm,\n",
    "        # )\n",
    "        batch_losses.append(loss)\n",
    "        lr_params = (decay_function, ssm_lr, lr, step, end_step, opt_config, lr_min)\n",
    "        state, step = update_learning_rate_per_step(lr_params, state)\n",
    "\n",
    "    # Return average loss over batches\n",
    "    return state, np.mean(np.array(batch_losses)), step\n",
    "\n",
    "def train(args):\n",
    "    \"\"\"\n",
    "    Main function to train over a certain number of epochs\n",
    "    \"\"\"\n",
    "\n",
    "    best_test_loss = 100000000\n",
    "    best_test_acc = -10000.0\n",
    "\n",
    "    ssm_size = args.ssm_size_base\n",
    "    ssm_lr = args.ssm_lr_base\n",
    "\n",
    "    # determine the size of initial blocks\n",
    "    block_size = int(ssm_size / args.blocks)\n",
    "\n",
    "    # Set global learning rate lr (e.g. encoders, etc.) as function of ssm_lr\n",
    "    lr = args.lr_factor * ssm_lr\n",
    "\n",
    "    # Set randomness...\n",
    "    print(\"[*] Setting Randomness...\")\n",
    "    key = random.PRNGKey(args.jax_seed)\n",
    "    init_rng, train_rng = random.split(key, num=2)\n",
    "\n",
    "    # Get dataset creation function\n",
    "    create_dataset_fn = Datasets[args.dataset]\n",
    "\n",
    "    # Dataset dependent logic\n",
    "    if args.dataset in [\"imdb-classification\", \"listops-classification\", \"aan-classification\"]:\n",
    "        padded = True\n",
    "        if args.dataset in [\"aan-classification\"]:\n",
    "            # Use retreival model for document matching\n",
    "            retrieval = True\n",
    "            print(\"Using retrieval model for document matching\")\n",
    "        else:\n",
    "            retrieval = False\n",
    "\n",
    "    else:\n",
    "        padded = False\n",
    "        retrieval = False\n",
    "\n",
    "    # For speech dataset\n",
    "    if args.dataset in [\"speech35-classification\"]:\n",
    "        speech = True\n",
    "        print(\"Will evaluate on both resolutions for speech task\")\n",
    "    else:\n",
    "        speech = False\n",
    "\n",
    "    # Create dataset...\n",
    "    init_rng, key = random.split(init_rng, num=2)\n",
    "    trainloader, valloader, testloader, aux_dataloaders, n_classes, seq_len, in_dim, train_size = \\\n",
    "        create_dataset_fn(args.dir_name, seed=args.jax_seed, bsz=args.bsz)\n",
    "\n",
    "    print(f\"[*] Starting S5 Training on `{args.dataset}` =>> Initializing...\")\n",
    "\n",
    "    # Initialize state matrix A using approximation to HiPPO-LegS matrix\n",
    "    Lambda, _, B, V, B_orig = make_DPLR_HiPPO(block_size)\n",
    "\n",
    "    if args.conj_sym:\n",
    "        block_size = block_size // 2\n",
    "        ssm_size = ssm_size // 2\n",
    "\n",
    "    Lambda = Lambda[:block_size]\n",
    "    V = V[:, :block_size]\n",
    "    Vc = V.conj().T\n",
    "\n",
    "    # If initializing state matrix A as block-diagonal, put HiPPO approximation\n",
    "    # on each block\n",
    "    Lambda = (Lambda * np.ones((args.blocks, block_size))).ravel()\n",
    "    V = block_diag(*([V] * args.blocks))\n",
    "    Vinv = block_diag(*([Vc] * args.blocks))\n",
    "\n",
    "    print(\"Lambda.shape={}\".format(Lambda.shape))\n",
    "    print(\"V.shape={}\".format(V.shape))\n",
    "    print(\"Vinv.shape={}\".format(Vinv.shape))\n",
    "\n",
    "    ssm_init_fn = init_S5SSM(H=args.d_model,\n",
    "                             P=ssm_size,\n",
    "                             Lambda_re_init=Lambda.real,\n",
    "                             Lambda_im_init=Lambda.imag,\n",
    "                             V=V,\n",
    "                             Vinv=Vinv,\n",
    "                             C_init=args.C_init,\n",
    "                             discretization=args.discretization,\n",
    "                             dt_min=args.dt_min,\n",
    "                             dt_max=args.dt_max,\n",
    "                             conj_sym=args.conj_sym,\n",
    "                             clip_eigs=args.clip_eigs,\n",
    "                             bidirectional=args.bidirectional)\n",
    "\n",
    "    if retrieval:\n",
    "        # Use retrieval head for AAN task\n",
    "        print(\"Using Retrieval head for {} task\".format(args.dataset))\n",
    "        model_cls = partial(\n",
    "            RetrievalModel,\n",
    "            ssm=ssm_init_fn,\n",
    "            d_output=n_classes,\n",
    "            d_model=args.d_model,\n",
    "            n_layers=args.n_layers,\n",
    "            padded=padded,\n",
    "            activation=args.activation_fn,\n",
    "            dropout=args.p_dropout,\n",
    "            prenorm=args.prenorm,\n",
    "            batchnorm=args.batchnorm,\n",
    "            bn_momentum=args.bn_momentum,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        model_cls = partial(\n",
    "            BatchClassificationModel,\n",
    "            ssm=ssm_init_fn,\n",
    "            d_output=n_classes,\n",
    "            d_model=args.d_model,\n",
    "            n_layers=args.n_layers,\n",
    "            padded=padded,\n",
    "            activation=args.activation_fn,\n",
    "            dropout=args.p_dropout,\n",
    "            mode=args.mode,\n",
    "            prenorm=args.prenorm,\n",
    "            batchnorm=args.batchnorm,\n",
    "            bn_momentum=args.bn_momentum,\n",
    "        )\n",
    "\n",
    "    # initialize training state\n",
    "    state = create_train_state(model_cls,\n",
    "                               init_rng,\n",
    "                               padded,\n",
    "                               retrieval,\n",
    "                               in_dim=in_dim,\n",
    "                               bsz=args.bsz,\n",
    "                               seq_len=seq_len,\n",
    "                               weight_decay=args.weight_decay,\n",
    "                               batchnorm=args.batchnorm,\n",
    "                               opt_config=args.opt_config,\n",
    "                               ssm_lr=ssm_lr,\n",
    "                               lr=lr,\n",
    "                               dt_global=args.dt_global)\n",
    "    # print(state.)\n",
    "    # Training Loop over epochs\n",
    "    step = 0  # for per step learning rate decay\n",
    "    steps_per_epoch = int(train_size / args.bsz)\n",
    "    for epoch in range(args.epochs):\n",
    "        print(f\"[*] Starting Training Epoch {epoch + 1}...\")\n",
    "\n",
    "        if epoch < args.warmup_end:\n",
    "            print(\"using linear warmup for epoch {}\".format(epoch + 1))\n",
    "            decay_function = linear_warmup\n",
    "            end_step = steps_per_epoch * args.warmup_end\n",
    "\n",
    "        elif args.cosine_anneal:\n",
    "            print(\"using cosine annealing for epoch {}\".format(epoch + 1))\n",
    "            decay_function = cosine_annealing\n",
    "            # for per step learning rate decay\n",
    "            end_step = steps_per_epoch * args.epochs - (steps_per_epoch * args.warmup_end)\n",
    "        else:\n",
    "            print(\"using constant lr for epoch {}\".format(epoch + 1))\n",
    "            decay_function = constant_lr\n",
    "            end_step = None\n",
    "\n",
    "        # TODO: Switch to letting Optax handle this.\n",
    "        #  Passing this around to manually handle per step learning rate decay.\n",
    "        lr_params = (decay_function, ssm_lr, lr, step, end_step, args.opt_config, args.lr_min)\n",
    "\n",
    "        train_rng, skey = random.split(train_rng)\n",
    "        state, train_loss, step = train_epoch(state,\n",
    "                                              skey,\n",
    "                                              model_cls,\n",
    "                                              trainloader,\n",
    "                                              seq_len,\n",
    "                                              in_dim,\n",
    "                                              args.batchnorm,\n",
    "                                              lr_params)"
   ],
   "id": "7694892575a7015c",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T23:48:56.277744Z",
     "start_time": "2025-01-16T23:48:55.173257Z"
    }
   },
   "cell_type": "code",
   "source": "train(args)",
   "id": "8387654f5dd18caf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Setting Randomness...\n",
      "[*] Generating MNIST Classification Dataset\n",
      "[*] Starting S5 Training on `mnist-classification` =>> Initializing...\n",
      "Lambda.shape=(128,)\n",
      "V.shape=(256, 128)\n",
      "Vinv.shape=(128, 256)\n",
      "configuring standard optimization setup\n",
      "[*] Trainable Parameters: 34122\n",
      "[*] Starting Training Epoch 1...\n",
      "using linear warmup for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3375 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 784, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3375 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: (16, 784)\n",
      "[[-1.41488810e-04  1.06206964e-04  8.66978808e-05 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [-4.08149390e-05  1.59114177e-04 -4.33817440e-05 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 1.47605155e-04 -9.43694104e-05 -9.91910783e-05 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [-5.14317217e-05  1.58796785e-04 -3.24650755e-05 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 1.57166272e-04 -2.47842181e-05 -1.45911908e-04 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [-1.56634182e-04  2.05286233e-05  1.47638697e-04 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[110], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[109], line 242\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m    239\u001B[0m lr_params \u001B[38;5;241m=\u001B[39m (decay_function, ssm_lr, lr, step, end_step, args\u001B[38;5;241m.\u001B[39mopt_config, args\u001B[38;5;241m.\u001B[39mlr_min)\n\u001B[1;32m    241\u001B[0m train_rng, skey \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39msplit(train_rng)\n\u001B[0;32m--> 242\u001B[0m state, train_loss, step \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    243\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mskey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    244\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mmodel_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    245\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mtrainloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    246\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mseq_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    247\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43min_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatchnorm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    249\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mlr_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[109], line 55\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(state, rng, model, trainloader, seq_len, in_dim, batchnorm, lr_params)\u001B[0m\n\u001B[1;32m     53\u001B[0m inputs, labels, integration_times \u001B[38;5;241m=\u001B[39m prep_batch(batch, seq_len, in_dim)\n\u001B[1;32m     54\u001B[0m rng, drop_rng \u001B[38;5;241m=\u001B[39m jax\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39msplit(rng)\n\u001B[0;32m---> 55\u001B[0m hiddens, param_grads, input_grads \u001B[38;5;241m=\u001B[39m hidden_gradients(\n\u001B[1;32m     56\u001B[0m     state\u001B[38;5;241m=\u001B[39mstate,\n\u001B[1;32m     57\u001B[0m     rng\u001B[38;5;241m=\u001B[39mrng,\n\u001B[1;32m     58\u001B[0m     batch_inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m     59\u001B[0m     batch_integration_timesteps\u001B[38;5;241m=\u001B[39mintegration_times,\n\u001B[1;32m     60\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     61\u001B[0m     batchnorm\u001B[38;5;241m=\u001B[39mbatchnorm\n\u001B[1;32m     62\u001B[0m )\n\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# print(\"THIS SEEMS TO WORK SOMEHOW\")\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# state, loss = hidden_gradient_step(\u001B[39;00m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;66;03m#     state,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;66;03m#     batchnorm,\u001B[39;00m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[1;32m     73\u001B[0m batch_losses\u001B[38;5;241m.\u001B[39mappend(loss)\n",
      "\u001B[0;31mTypeError\u001B[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T23:38:45.234430Z",
     "start_time": "2025-01-15T23:38:45.223832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# best_test_loss = 100000000\n",
    "# best_test_acc = -10000.0\n",
    "# \n",
    "# ssm_size = args.ssm_size_base\n",
    "# ssm_lr = args.ssm_lr_base\n",
    "# \n",
    "# # determine the size of initial blocks\n",
    "# block_size = int(ssm_size / args.blocks)\n",
    "# \n",
    "# # Set global learning rate lr (e.g. encoders, etc.) as function of ssm_lr\n",
    "# lr = args.lr_factor * ssm_lr\n",
    "# \n",
    "# # Set randomness...\n",
    "# print(\"[*] Setting Randomness...\")\n",
    "# key = random.PRNGKey(args.jax_seed)\n",
    "# init_rng, train_rng = random.split(key, num=2)\n",
    "# \n",
    "# # Get dataset creation function\n",
    "# create_dataset_fn = Datasets[args.dataset]\n",
    "# \n",
    "# # Dataset dependent logic\n",
    "# if args.dataset in [\"imdb-classification\", \"listops-classification\", \"aan-classification\"]:\n",
    "#     padded = True\n",
    "#     if args.dataset in [\"aan-classification\"]:\n",
    "#         # Use retreival model for document matching\n",
    "#         retrieval = True\n",
    "#         print(\"Using retrieval model for document matching\")\n",
    "#     else:\n",
    "#         retrieval = False\n",
    "# \n",
    "# else:\n",
    "#     padded = False\n",
    "#     retrieval = False\n",
    "# \n",
    "# # For speech dataset\n",
    "# if args.dataset in [\"speech35-classification\"]:\n",
    "#     speech = True\n",
    "#     print(\"Will evaluate on both resolutions for speech task\")\n",
    "# else:\n",
    "#     speech = False\n",
    "# \n",
    "# # Create dataset...\n",
    "# init_rng, key = random.split(init_rng, num=2)\n",
    "# trainloader, valloader, testloader, aux_dataloaders, n_classes, seq_len, in_dim, train_size = \\\n",
    "#     create_dataset_fn(args.dir_name, seed=args.jax_seed, bsz=args.bsz)\n",
    "# \n",
    "# print(f\"[*] Starting S5 Training on `{args.dataset}` =>> Initializing...\")\n",
    "# \n",
    "# # Initialize state matrix A using approximation to HiPPO-LegS matrix\n",
    "# Lambda, _, B, V, B_orig = make_DPLR_HiPPO(block_size)\n",
    "# \n",
    "# if args.conj_sym:\n",
    "#     block_size = block_size // 2\n",
    "#     ssm_size = ssm_size // 2\n",
    "# \n",
    "# Lambda = Lambda[:block_size]\n",
    "# V = V[:, :block_size]\n",
    "# Vc = V.conj().T\n",
    "# \n",
    "# # If initializing state matrix A as block-diagonal, put HiPPO approximation\n",
    "# # on each block\n",
    "# Lambda = (Lambda * np.ones((args.blocks, block_size))).ravel()\n",
    "# V = block_diag(*([V] * args.blocks))\n",
    "# Vinv = block_diag(*([Vc] * args.blocks))\n",
    "# \n",
    "# print(\"Lambda.shape={}\".format(Lambda.shape))\n",
    "# print(\"V.shape={}\".format(V.shape))\n",
    "# print(\"Vinv.shape={}\".format(Vinv.shape))\n",
    "# \n",
    "# ssm_init_fn = init_S5SSM(H=args.d_model,\n",
    "#                          P=ssm_size,\n",
    "#                          Lambda_re_init=Lambda.real,\n",
    "#                          Lambda_im_init=Lambda.imag,\n",
    "#                          V=V,\n",
    "#                          Vinv=Vinv,\n",
    "#                          C_init=args.C_init,\n",
    "#                          discretization=args.discretization,\n",
    "#                          dt_min=args.dt_min,\n",
    "#                          dt_max=args.dt_max,\n",
    "#                          conj_sym=args.conj_sym,\n",
    "#                          clip_eigs=args.clip_eigs,\n",
    "#                          bidirectional=args.bidirectional)\n",
    "# \n",
    "# if retrieval:\n",
    "#     # Use retrieval head for AAN task\n",
    "#     print(\"Using Retrieval head for {} task\".format(args.dataset))\n",
    "#     model_cls = partial(\n",
    "#         RetrievalModel,\n",
    "#         ssm=ssm_init_fn,\n",
    "#         d_output=n_classes,\n",
    "#         d_model=args.d_model,\n",
    "#         n_layers=args.n_layers,\n",
    "#         padded=padded,\n",
    "#         activation=args.activation_fn,\n",
    "#         dropout=args.p_dropout,\n",
    "#         prenorm=args.prenorm,\n",
    "#         batchnorm=args.batchnorm,\n",
    "#         bn_momentum=args.bn_momentum,\n",
    "#     )\n",
    "# \n",
    "# else:\n",
    "#     model_cls = partial(\n",
    "#         BatchClassificationModel,\n",
    "#         ssm=ssm_init_fn,\n",
    "#         d_output=n_classes,\n",
    "#         d_model=args.d_model,\n",
    "#         n_layers=args.n_layers,\n",
    "#         padded=padded,\n",
    "#         activation=args.activation_fn,\n",
    "#         dropout=args.p_dropout,\n",
    "#         mode=args.mode,\n",
    "#         prenorm=args.prenorm,\n",
    "#         batchnorm=args.batchnorm,\n",
    "#         bn_momentum=args.bn_momentum,\n",
    "#     )\n",
    "# \n",
    "# # initialize training state\n",
    "# state = create_train_state(model_cls,\n",
    "#                            init_rng,\n",
    "#                            padded,\n",
    "#                            retrieval,\n",
    "#                            in_dim=in_dim,\n",
    "#                            bsz=args.bsz,\n",
    "#                            seq_len=seq_len,\n",
    "#                            weight_decay=args.weight_decay,\n",
    "#                            batchnorm=args.batchnorm,\n",
    "#                            opt_config=args.opt_config,\n",
    "#                            ssm_lr=ssm_lr,\n",
    "#                            lr=lr,\n",
    "#                            dt_global=args.dt_global)\n",
    "# \n",
    "# model = model_cls(training=True)\n",
    "# batch_losses = []\n",
    "# train_rng, skey = random.split(train_rng)\n",
    "# lr_params = (decay_function, ssm_lr, lr, step, end_step, args.opt_config, args.lr_min)\n",
    "# decay_function, ssm_lr, lr, step, end_step, opt_config, lr_min = lr_params\n",
    "# \n",
    "# for batch_idx, batch in enumerate(itertools.islice(tqdm(trainloader), 10)):\n",
    "#     inputs, labels, integration_times = prep_batch(batch, seq_len, in_dim)\n",
    "#     \n",
    "#     rng, drop_rng = jax.random.split(skey)\n",
    "#     state, loss = train_step(\n",
    "#         state,\n",
    "#         drop_rng,\n",
    "#         inputs,\n",
    "#         labels,\n",
    "#         integration_times,\n",
    "#         model,\n",
    "#         args.batchnorm,\n",
    "#     )\n",
    "#     # time_indices = np.array([0, seq_len//2, seq_len-1]) # this will be the array of the indices within the mnist image (len 784), we will monitor the gradient at the start, beginning, and end\n",
    "#     # grad_value = gradient_monitoring_hook(\n",
    "#     #         state=state,\n",
    "#     #         batch={'inputs': inputs, 'labels': labels},\n",
    "#     #         time_indices=time_indices,\n",
    "#     #         state_idx=0  # Monitor first hidden state dimension\n",
    "#     #     )\n",
    "#     batch_losses.append(loss)\n",
    "#     lr_params = (decay_function, ssm_lr, lr, step, end_step, opt_config, lr_min)\n",
    "#     state, step = update_learning_rate_per_step(lr_params, state)"
   ],
   "id": "2123e530ad7174e9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m -> \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(d)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Assuming `state.params` is the dictionary:\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m explore_dict_structure_as_directory(\u001B[43mstate\u001B[49m\u001B[38;5;241m.\u001B[39mparams)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'state' is not defined"
     ]
    }
   ],
   "execution_count": 4,
   "source": [
    "def explore_dict_structure_as_directory(d, path=\"\"):\n",
    "    if isinstance(d, dict):\n",
    "        for key, value in d.items():\n",
    "            new_path = f\"{path}/{key}\" if path else key\n",
    "            print(new_path)\n",
    "            explore_dict_structure_as_directory(value, new_path)\n",
    "    else:\n",
    "        print(f\"{path} -> {type(d).__name__}\")\n",
    "\n",
    "# Assuming `state.params` is the dictionary:\n",
    "explore_dict_structure_as_directory(state.params)"
   ],
   "id": "fc8d0093ff78348c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "state.model",
   "id": "c02c3727ca37f1e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": "",
   "id": "cb218da57fe09832"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": "",
   "id": "ca44bd0d25d12a1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": "",
   "id": "500dfddb5b9d3d58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5,
   "source": "args.batchnorm",
   "id": "501d863762124a62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T23:35:26.218653Z",
     "start_time": "2025-01-15T23:35:20.463355Z"
    }
   },
   "cell_type": "code",
   "source": "train(args)",
   "id": "f458bc9f8710b27b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Setting Randomness...\n",
      "[*] Generating MNIST Classification Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakub/miniconda3/envs/jax-env/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/jakub/miniconda3/envs/jax-env/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <9DBE5D5C-AC87-30CA-96DA-F5BC116EDA2B> /Users/jakub/miniconda3/envs/jax-env/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <D400622C-0C6B-3AE1-AB45-F1D0BF19B384> /Users/jakub/miniconda3/envs/jax-env/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Starting S5 Training on `mnist-classification` =>> Initializing...\n",
      "Lambda.shape=(128,)\n",
      "V.shape=(256, 128)\n",
      "Vinv.shape=(128, 256)\n",
      "configuring standard optimization setup\n",
      "[*] Trainable Parameters: 34122\n",
      "{'decoder': {'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), 'kernel': Array([[ 2.81169564e-01, -2.19013482e-01, -6.91609159e-02,\n",
      "        -1.44792110e-01, -9.88199711e-02,  1.05479829e-01,\n",
      "         1.18012309e-01, -6.14414811e-02,  1.42612875e-01,\n",
      "         3.11395880e-02],\n",
      "       [-1.67155378e-02,  1.40935361e-01, -1.52113259e-01,\n",
      "        -4.82966378e-02, -2.34872401e-01, -2.13007480e-02,\n",
      "        -2.57653236e-01, -1.33985296e-01,  1.71239257e-01,\n",
      "        -9.71533731e-02],\n",
      "       [-2.81250775e-01,  5.37459664e-02,  7.20960051e-02,\n",
      "         1.14900894e-01,  1.17362468e-02, -1.00356825e-01,\n",
      "         2.52808668e-02, -1.68488190e-01, -9.68281925e-03,\n",
      "        -7.25319237e-02],\n",
      "       [-1.78734630e-01,  1.10142782e-01, -1.11236036e-01,\n",
      "        -1.16073810e-01,  1.91563502e-01, -9.52764675e-02,\n",
      "         2.57268623e-02,  5.74564114e-02, -1.84900701e-01,\n",
      "        -7.56916702e-02],\n",
      "       [ 1.99764609e-01, -1.55985296e-01, -8.46576914e-02,\n",
      "         1.68439031e-01, -8.51698965e-02,  2.07439482e-01,\n",
      "        -4.78650890e-02,  1.36750326e-01, -5.36584221e-02,\n",
      "        -4.66426164e-02],\n",
      "       [-1.30609587e-01,  5.12054786e-02,  2.51233354e-02,\n",
      "         2.17320442e-01,  2.10632920e-01, -7.02397600e-02,\n",
      "         5.59511669e-02, -3.26504409e-02, -2.14193091e-01,\n",
      "         1.41673148e-01],\n",
      "       [-1.43963084e-01,  5.60000949e-02,  1.11435398e-01,\n",
      "        -3.20749474e-03, -4.80291173e-02,  1.14227757e-01,\n",
      "        -5.28023206e-02, -9.62188393e-02, -3.11610959e-02,\n",
      "         2.23181248e-01],\n",
      "       [-3.66180502e-02, -1.50976796e-02,  1.69056132e-01,\n",
      "         5.50833391e-03, -2.01647401e-01, -5.29988818e-02,\n",
      "         2.39916354e-01,  2.37889290e-01,  1.05736680e-01,\n",
      "         1.28490984e-01],\n",
      "       [ 9.33194757e-02, -5.15698120e-02,  7.34843761e-02,\n",
      "         9.29875672e-02,  1.99613914e-01,  6.53140768e-02,\n",
      "         9.44375247e-03, -1.18540311e-02,  6.83891028e-02,\n",
      "        -5.87842390e-02],\n",
      "       [-1.41735477e-02, -1.73064426e-01, -1.11629650e-01,\n",
      "        -2.09430769e-01,  5.70031675e-03, -4.90794964e-02,\n",
      "        -1.22394882e-01,  1.52305737e-01,  2.00664014e-01,\n",
      "         4.75935116e-02],\n",
      "       [ 1.07623622e-01, -1.32457465e-01,  1.17143271e-02,\n",
      "         1.85873419e-01,  7.02033117e-02,  7.33973384e-02,\n",
      "        -1.87637493e-01,  1.28437728e-01,  7.77276605e-02,\n",
      "        -1.88551366e-01],\n",
      "       [-7.24683106e-02,  1.48627073e-01,  1.28408998e-01,\n",
      "         8.90891328e-02,  1.12011395e-01, -8.05439577e-02,\n",
      "        -4.41474468e-02, -1.30090564e-01, -4.04447131e-02,\n",
      "        -2.22685382e-01],\n",
      "       [ 5.47619015e-02, -5.66802826e-03, -5.00330329e-02,\n",
      "         1.95732042e-02, -1.29758760e-01,  1.07868865e-01,\n",
      "         6.63142325e-03, -3.70999314e-02, -4.52267751e-02,\n",
      "        -9.97612178e-02],\n",
      "       [-6.04493357e-02, -4.27429527e-02, -1.59860253e-01,\n",
      "        -1.53694868e-01, -2.67043203e-01, -7.86201581e-02,\n",
      "        -5.16324975e-02, -9.88630801e-02,  1.09349370e-01,\n",
      "         1.24068730e-01],\n",
      "       [-5.99890538e-02,  1.23303518e-01,  1.51463196e-01,\n",
      "         2.15588901e-02, -1.65016994e-01,  3.06276921e-02,\n",
      "        -1.47944644e-01,  3.80548686e-02, -3.52311842e-02,\n",
      "        -1.34739265e-01],\n",
      "       [-6.88225701e-02, -7.83348829e-02,  7.18360469e-02,\n",
      "         2.86309868e-02,  8.30932111e-02,  7.06526935e-02,\n",
      "         2.11882308e-01,  8.95578340e-02, -1.38760507e-01,\n",
      "        -2.17797101e-01],\n",
      "       [-2.11898953e-01,  6.48446009e-02, -1.13262191e-01,\n",
      "        -4.18598093e-02,  5.66025078e-02,  1.22033149e-01,\n",
      "        -1.67895760e-02,  1.48148492e-01, -1.09491162e-01,\n",
      "        -7.20226765e-02],\n",
      "       [-8.14613923e-02, -1.02697536e-01, -2.90254969e-03,\n",
      "         2.74955451e-01,  1.41131803e-02, -4.67426777e-02,\n",
      "         1.64436936e-01, -3.80017236e-02, -3.19657102e-02,\n",
      "         6.93120807e-02],\n",
      "       [-1.24097094e-01,  1.07000239e-01, -3.08101997e-02,\n",
      "         1.43219367e-01, -9.19672772e-02,  8.16262439e-02,\n",
      "        -2.08713830e-01, -2.14435607e-01, -3.09910644e-02,\n",
      "         8.08968693e-02],\n",
      "       [-5.52114770e-02,  9.75338295e-02, -6.35286495e-02,\n",
      "        -9.65677872e-02, -1.41722664e-01, -5.26610278e-02,\n",
      "         7.68822730e-02,  7.47594535e-02,  1.38382137e-01,\n",
      "         1.57266837e-02],\n",
      "       [-1.49568999e-02,  5.02870567e-02,  1.82957243e-04,\n",
      "         8.54447410e-02,  9.88766085e-03,  8.18174258e-02,\n",
      "         7.96111450e-02, -8.54556188e-02,  6.14796532e-03,\n",
      "         1.16810061e-01],\n",
      "       [-2.47860417e-01, -2.21449688e-01,  6.38533235e-02,\n",
      "        -2.69032836e-01, -8.35284665e-02,  5.51281348e-02,\n",
      "        -4.48127128e-02,  1.03261232e-01,  1.44170690e-02,\n",
      "         3.86707708e-02],\n",
      "       [ 1.82126597e-01,  2.16091618e-01,  1.62819520e-01,\n",
      "         1.15930676e-01,  7.45698065e-02,  1.40603095e-01,\n",
      "         1.05965473e-01, -7.97299296e-03, -6.98505640e-02,\n",
      "         1.48746833e-01],\n",
      "       [-1.37266740e-01,  1.72717035e-01, -2.34975219e-01,\n",
      "        -2.11107433e-02, -1.70198306e-01, -1.19861793e-02,\n",
      "        -2.01781821e-02, -4.14831080e-02, -3.52209434e-02,\n",
      "        -1.54073969e-01],\n",
      "       [-2.08714865e-02, -4.16747443e-02, -3.32919024e-02,\n",
      "         2.35629097e-01, -6.53562993e-02, -1.23926446e-01,\n",
      "        -3.49774584e-02, -1.92485332e-01, -3.41271836e-04,\n",
      "         1.47922650e-01],\n",
      "       [ 8.58605057e-02,  2.24348336e-01,  6.54239729e-02,\n",
      "        -1.46807984e-01,  7.97965154e-02,  1.27331644e-01,\n",
      "        -2.20942497e-01,  1.23465188e-01, -1.76629916e-01,\n",
      "         6.19491600e-02],\n",
      "       [-3.57309915e-02,  2.74689104e-02, -1.64884076e-01,\n",
      "         3.93094160e-02, -1.93104118e-01,  1.41512156e-01,\n",
      "         1.00281797e-01,  2.21071020e-02, -1.56249195e-01,\n",
      "         1.98698208e-01],\n",
      "       [ 4.54629306e-03,  2.33142555e-01, -7.15440186e-03,\n",
      "         7.31666535e-02, -4.53274585e-02,  1.47120938e-01,\n",
      "         2.29701027e-01,  2.16955736e-01,  2.76684433e-01,\n",
      "        -9.86956656e-02],\n",
      "       [ 6.50670081e-02,  1.00688159e-01,  6.27197325e-02,\n",
      "         1.85978740e-01, -2.49108478e-01,  1.42469496e-01,\n",
      "         9.15412679e-02, -5.80911105e-03, -3.63899232e-03,\n",
      "        -7.63302743e-02],\n",
      "       [-2.21385956e-01, -8.35267678e-02,  3.69990878e-02,\n",
      "        -3.92576680e-02, -1.29070869e-02, -5.56457639e-02,\n",
      "         1.44015834e-01, -9.42565426e-02, -3.95093635e-02,\n",
      "         2.73053143e-02],\n",
      "       [ 8.74413252e-02,  2.62053072e-01,  6.04525246e-02,\n",
      "         8.47158432e-02,  1.19114257e-01,  9.00887027e-02,\n",
      "         1.12025991e-01, -2.79521972e-01,  5.04168347e-02,\n",
      "         4.38260399e-02],\n",
      "       [ 6.69661388e-02, -5.82118481e-02,  4.66001481e-02,\n",
      "        -5.46462797e-02, -1.16340511e-01,  1.20901987e-01,\n",
      "        -1.20381035e-01,  2.42250413e-01, -2.09729746e-02,\n",
      "         3.66000719e-02],\n",
      "       [-2.47191135e-02,  1.82437748e-02, -1.92256153e-01,\n",
      "        -8.42736959e-02, -1.73614085e-01,  6.21103197e-02,\n",
      "        -1.79264456e-01,  8.50679129e-02, -2.36202642e-01,\n",
      "        -4.12893072e-02],\n",
      "       [-1.75820570e-02,  2.03198552e-01,  2.12962806e-01,\n",
      "        -1.01221226e-01,  1.53217185e-02,  3.20924260e-02,\n",
      "        -2.31802434e-01, -1.04967698e-01, -8.43624473e-02,\n",
      "        -2.33907346e-02],\n",
      "       [ 1.33423835e-01, -1.03272736e-01, -1.49449900e-01,\n",
      "        -1.72806144e-01,  9.83208194e-02, -2.54644006e-01,\n",
      "        -4.02294695e-02, -1.03671946e-01, -6.01161644e-03,\n",
      "        -2.15221956e-01],\n",
      "       [-5.26708178e-03,  5.24544455e-02, -3.51620205e-02,\n",
      "        -1.45207807e-01, -7.39231408e-02,  1.21419825e-01,\n",
      "         4.40760814e-02,  1.31474257e-01,  3.96885686e-02,\n",
      "         1.54976755e-01],\n",
      "       [ 1.20978072e-01, -1.30554616e-01,  1.71399042e-01,\n",
      "        -1.46424785e-01,  1.35387927e-01,  1.62105083e-01,\n",
      "         9.87844169e-02,  1.70405835e-01,  1.54760823e-01,\n",
      "         2.77981497e-02],\n",
      "       [-1.88932553e-01, -1.37674123e-01,  8.63988623e-02,\n",
      "        -1.41660303e-01,  1.18409127e-01, -7.93074742e-02,\n",
      "        -1.97814822e-01, -6.26884177e-02,  2.70350933e-01,\n",
      "        -9.35064629e-02],\n",
      "       [ 1.61751539e-01, -8.42010416e-03,  1.04367413e-01,\n",
      "        -4.26893160e-02,  2.62920588e-01, -5.71953468e-02,\n",
      "         1.48127705e-01,  1.77370340e-01,  1.79898009e-01,\n",
      "         1.07384093e-01],\n",
      "       [ 7.13487938e-02,  1.17171191e-01,  1.76696301e-01,\n",
      "        -1.29683077e-01, -8.10198188e-02,  6.45048022e-02,\n",
      "        -6.44671842e-02, -2.03512356e-01,  3.37243900e-02,\n",
      "        -6.77830819e-03],\n",
      "       [-2.01737583e-02,  3.27182747e-02,  4.56448942e-02,\n",
      "         1.24015391e-01, -1.17245235e-01, -5.58387525e-02,\n",
      "         7.43289143e-02, -1.60025775e-01, -1.05379988e-02,\n",
      "         1.09102622e-01],\n",
      "       [ 1.02329977e-01, -1.88640043e-01,  7.78340027e-02,\n",
      "        -1.19389653e-01, -1.04632773e-01,  2.34532245e-02,\n",
      "         7.35974833e-02,  2.78400540e-01, -2.11289346e-01,\n",
      "         1.28840515e-02],\n",
      "       [ 7.80605227e-02, -4.83396649e-02,  1.39423504e-01,\n",
      "         2.44692594e-01,  2.05010518e-01, -1.54976407e-02,\n",
      "        -1.01957388e-01,  4.04055044e-02,  2.96974555e-02,\n",
      "        -7.35914800e-03],\n",
      "       [ 1.12522349e-01,  1.66495219e-01,  8.75180364e-02,\n",
      "         1.71214044e-01,  9.04485360e-02,  1.21139109e-01,\n",
      "        -3.58352512e-02,  8.41678977e-02,  6.25794306e-02,\n",
      "        -4.00849022e-02],\n",
      "       [ 8.31898898e-02,  1.06998287e-01,  8.37930292e-02,\n",
      "        -3.90073583e-02,  3.48155089e-02, -1.73734557e-02,\n",
      "         2.04927266e-01,  2.97591388e-02, -6.50949636e-03,\n",
      "         9.88555849e-02],\n",
      "       [ 6.78196773e-02, -2.60663122e-01, -1.07337832e-02,\n",
      "        -1.35973273e-02,  1.48094922e-01,  7.52774999e-02,\n",
      "         7.51766935e-02,  1.03304297e-01,  5.90132475e-02,\n",
      "         5.22397794e-02],\n",
      "       [ 5.74242249e-02, -2.57407054e-02, -1.52777076e-01,\n",
      "        -7.95263276e-02,  1.82431445e-01,  5.31909354e-02,\n",
      "         2.31866106e-01, -5.38798571e-02, -2.02553973e-01,\n",
      "         1.43094003e-01],\n",
      "       [ 7.81356618e-02, -1.15558848e-01, -1.32541716e-01,\n",
      "        -8.14820230e-02,  9.95064974e-02,  2.48987824e-01,\n",
      "        -1.33787738e-02, -1.39076635e-02, -1.07109874e-01,\n",
      "         2.56259143e-01],\n",
      "       [-1.15385793e-01, -1.83426335e-01,  8.22405741e-02,\n",
      "         9.34669077e-02,  9.10427719e-02,  2.61874571e-02,\n",
      "         1.86802164e-01,  6.56327754e-02, -7.68271461e-02,\n",
      "        -1.13224216e-01],\n",
      "       [-1.44748598e-01,  3.71115059e-02,  3.32200825e-02,\n",
      "        -1.15712978e-01,  1.42748088e-01,  2.64760971e-01,\n",
      "         8.38571787e-02,  9.05537326e-03, -1.68278590e-01,\n",
      "         6.89151809e-02],\n",
      "       [ 1.37621373e-01,  1.45731822e-01, -6.23111017e-02,\n",
      "         6.47743046e-02,  5.09142503e-03, -1.19720377e-01,\n",
      "        -2.02113017e-01, -1.69286057e-01,  3.56574506e-02,\n",
      "         7.31855854e-02],\n",
      "       [-6.77355230e-02, -8.56948644e-02,  4.76345327e-03,\n",
      "        -1.64345764e-02,  1.95462897e-01,  2.66158700e-01,\n",
      "         1.32703483e-01,  1.40982673e-01,  2.11289242e-01,\n",
      "        -1.24828629e-01],\n",
      "       [ 7.60711581e-02, -5.63662201e-02, -1.76462680e-01,\n",
      "         1.54047355e-01,  2.49701589e-02, -2.32283175e-01,\n",
      "        -2.37085283e-01,  3.32032144e-02, -1.98508173e-01,\n",
      "        -1.82286128e-01],\n",
      "       [-1.37200758e-01,  5.84526025e-02, -1.41142100e-01,\n",
      "        -1.37073889e-01,  1.03176601e-01,  1.46802753e-01,\n",
      "         2.13344797e-01,  9.15149376e-02, -8.81321803e-02,\n",
      "        -2.74615258e-01],\n",
      "       [-4.19391803e-02,  2.23177344e-01,  2.79245585e-01,\n",
      "        -1.06435224e-01,  5.12343682e-02, -1.67390242e-01,\n",
      "        -2.23865792e-01, -3.40952426e-02, -2.08626799e-02,\n",
      "        -2.34865695e-01],\n",
      "       [ 3.44429910e-02, -1.46991506e-01,  4.37289700e-02,\n",
      "        -4.60830629e-02, -5.37536256e-02, -1.29392728e-01,\n",
      "         1.04807869e-01,  5.49958013e-02,  2.82011572e-02,\n",
      "        -3.99028286e-02],\n",
      "       [-8.43715668e-03, -5.69144264e-02,  6.59141596e-03,\n",
      "         1.55159727e-01, -1.27669200e-01,  5.15416302e-02,\n",
      "         3.49148773e-02,  1.00036353e-01,  2.35492542e-01,\n",
      "        -1.79845165e-03],\n",
      "       [-8.64240751e-02, -4.79813591e-02, -1.48735978e-02,\n",
      "        -2.44531687e-02, -1.30351886e-01, -9.46058705e-02,\n",
      "         1.55024573e-01,  1.00639993e-02, -1.91215068e-01,\n",
      "         9.57423300e-02],\n",
      "       [-9.21521112e-02, -1.47710994e-01, -1.82558317e-02,\n",
      "        -2.48115510e-01, -1.21188238e-01,  1.83372259e-01,\n",
      "        -5.30466326e-02,  4.50170115e-02,  4.02523158e-03,\n",
      "         8.29774514e-02],\n",
      "       [ 8.64359811e-02, -8.30464810e-02,  9.60489139e-02,\n",
      "         8.60229321e-03, -9.54710096e-02, -6.78380355e-02,\n",
      "         1.38610676e-01,  4.17561159e-02,  1.35768205e-01,\n",
      "         6.96040317e-02],\n",
      "       [-6.20886460e-02, -2.48201743e-01, -2.10032240e-01,\n",
      "         1.41391560e-01, -1.93182200e-01, -2.72936493e-01,\n",
      "         6.08868375e-02,  1.39228716e-01, -3.96238361e-03,\n",
      "         6.36302959e-03],\n",
      "       [-1.70825809e-01,  1.94134951e-01,  4.96410131e-02,\n",
      "        -4.41394076e-02, -3.23438115e-04,  2.44387910e-01,\n",
      "        -2.76616484e-01, -1.50714621e-01,  1.09232068e-02,\n",
      "         7.38986433e-02],\n",
      "       [ 1.44424126e-01, -8.60387739e-03,  1.93623036e-01,\n",
      "         3.59447598e-02, -4.19775359e-02, -4.32911627e-02,\n",
      "        -8.97211283e-02,  9.55091324e-03,  1.81676000e-01,\n",
      "        -7.04458356e-02],\n",
      "       [ 2.13881701e-01, -6.85939640e-02, -8.92220214e-02,\n",
      "        -1.25593558e-01, -1.73669085e-01,  1.34299383e-01,\n",
      "         1.05441451e-01, -4.86041885e-03, -1.10468522e-01,\n",
      "         3.87195162e-02]], dtype=float32)}, 'encoder': {'encoder': {'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), 'kernel': Array([[-0.48310804,  1.939159  ,  0.3525804 ,  0.69534206, -0.3085025 ,\n",
      "        -0.9597689 , -0.00890233,  0.8551901 , -0.56573784, -0.24191518,\n",
      "         0.50144225, -0.48176908,  0.9688914 ,  0.90971935, -0.02138253,\n",
      "        -0.5771752 , -0.58817863,  1.5500565 , -0.24981323, -0.2206929 ,\n",
      "        -0.18887398, -0.28741947, -1.781515  ,  1.0037067 ,  0.11262684,\n",
      "         0.2039275 ,  0.32520375,  0.5356777 , -0.13533735,  0.97961396,\n",
      "        -1.0765827 ,  0.99503475, -0.9656912 ,  0.18011294,  0.6141875 ,\n",
      "         1.095928  , -0.7583404 ,  0.44985092,  0.44173974,  0.7795444 ,\n",
      "        -0.20906039, -0.2111179 ,  0.9532813 ,  1.4825556 , -1.6374549 ,\n",
      "         0.8823481 ,  1.1328017 ,  0.6545107 , -0.46862987,  2.2442706 ,\n",
      "        -0.6604372 , -1.3604285 , -0.55012006,  0.8474008 , -0.4240328 ,\n",
      "        -1.036847  ,  0.727038  ,  0.09966531, -0.9884016 , -1.2418264 ,\n",
      "        -1.8738363 , -0.59469384, -1.0118521 , -1.190177  ]],      dtype=float32)}, 'layers_0': {'norm': {'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), 'scale': Array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)}, 'seq': {'B': Array([[[ 0.02691589, -0.06132279],\n",
      "        [ 0.01671908,  0.01707276],\n",
      "        [ 0.04818448,  0.00542227],\n",
      "        ...,\n",
      "        [-0.02194314, -0.03493608],\n",
      "        [-0.0681112 ,  0.03289638],\n",
      "        [ 0.00996993,  0.00968263]],\n",
      "\n",
      "       [[ 0.0430832 , -0.06490883],\n",
      "        [ 0.02349202,  0.00495682],\n",
      "        [-0.02749855,  0.01875914],\n",
      "        ...,\n",
      "        [-0.00065496,  0.01036356],\n",
      "        [ 0.01658698,  0.03091911],\n",
      "        [ 0.040905  , -0.04898282]],\n",
      "\n",
      "       [[-0.06674212,  0.04302018],\n",
      "        [-0.10605828,  0.03269763],\n",
      "        [-0.00544339,  0.00975811],\n",
      "        ...,\n",
      "        [-0.05048247,  0.00233315],\n",
      "        [-0.01369852, -0.03375745],\n",
      "        [-0.02867818,  0.05650823]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.00566938, -0.03709574],\n",
      "        [-0.0066183 ,  0.00031657],\n",
      "        [ 0.01632521,  0.06855249],\n",
      "        ...,\n",
      "        [-0.00809717, -0.00544986],\n",
      "        [ 0.0063796 ,  0.07763317],\n",
      "        [ 0.01993465,  0.03060848]],\n",
      "\n",
      "       [[-0.04361293,  0.03763419],\n",
      "        [-0.08066903,  0.00338943],\n",
      "        [-0.02995955, -0.00756249],\n",
      "        ...,\n",
      "        [-0.06074861, -0.09537995],\n",
      "        [-0.02811685,  0.02439652],\n",
      "        [-0.00754216,  0.06556093]],\n",
      "\n",
      "       [[-0.03749697, -0.00173306],\n",
      "        [-0.01929162,  0.01791835],\n",
      "        [-0.02183646, -0.03041833],\n",
      "        ...,\n",
      "        [ 0.04700394,  0.0225177 ],\n",
      "        [ 0.05881975, -0.02111234],\n",
      "        [ 0.07100056,  0.002761  ]]], dtype=float32), 'C': Array([[[-1.1366363 ,  0.6218689 ],\n",
      "        [-0.10066037,  0.15268596],\n",
      "        [-1.1753994 , -0.36602366],\n",
      "        ...,\n",
      "        [-0.44091144,  0.43131006],\n",
      "        [ 0.71755505,  0.17039698],\n",
      "        [ 0.9360179 , -0.486236  ]],\n",
      "\n",
      "       [[ 0.0352947 ,  0.28285652],\n",
      "        [ 0.760041  ,  1.1739621 ],\n",
      "        [-0.19611254, -0.99946934],\n",
      "        ...,\n",
      "        [-0.17385267, -0.6397676 ],\n",
      "        [ 0.14543107, -0.6849612 ],\n",
      "        [ 0.4070162 , -0.91584426]],\n",
      "\n",
      "       [[ 0.24552356, -1.7618101 ],\n",
      "        [ 0.70419693,  1.0577222 ],\n",
      "        [-0.02404803, -0.61025834],\n",
      "        ...,\n",
      "        [ 0.07535534,  0.26319337],\n",
      "        [-0.14516336,  1.5913693 ],\n",
      "        [ 0.06554559, -0.19729593]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.57150453,  0.19654615],\n",
      "        [-0.5538939 , -0.7914728 ],\n",
      "        [ 0.9083557 , -0.0805805 ],\n",
      "        ...,\n",
      "        [-1.4973526 , -0.5687561 ],\n",
      "        [-0.7240515 ,  1.2439557 ],\n",
      "        [-1.3963919 ,  0.82610786]],\n",
      "\n",
      "       [[ 0.03932799,  0.328232  ],\n",
      "        [ 0.6289614 ,  0.420387  ],\n",
      "        [ 0.12768267,  0.09688114],\n",
      "        ...,\n",
      "        [-0.02217793, -0.801438  ],\n",
      "        [-1.0367029 ,  0.5452388 ],\n",
      "        [ 0.01863677,  0.94607115]],\n",
      "\n",
      "       [[-0.83278716, -1.0704379 ],\n",
      "        [ 0.4643606 ,  0.503057  ],\n",
      "        [-0.06619676, -0.88639146],\n",
      "        ...,\n",
      "        [ 0.6886781 ,  0.1923225 ],\n",
      "        [-1.5278338 ,  0.36495322],\n",
      "        [-0.36642838,  0.76587117]]], dtype=float32), 'D': Array([-0.5489837 ,  0.761681  , -2.2815435 , -0.16214968,  0.5626498 ,\n",
      "       -0.94783556, -0.97850484,  0.6482844 , -1.2011354 , -1.5518414 ,\n",
      "       -0.6296067 ,  0.5789034 ,  0.05411617,  0.80276513, -1.6558183 ,\n",
      "       -0.6433024 , -0.00614775, -0.3077091 ,  0.07879405, -0.36592242,\n",
      "        0.9185063 , -0.7712749 , -0.550394  ,  2.0350628 ,  0.60789174,\n",
      "       -0.08142022,  0.6945121 ,  0.52923506, -1.0489923 , -0.61018646,\n",
      "       -0.17604797, -0.25776446, -0.24202418,  0.5415567 , -1.7117512 ,\n",
      "       -0.24507087,  0.05512857,  0.563723  , -0.04044937,  0.5556459 ,\n",
      "        1.4069995 , -0.4349617 , -1.6619759 , -0.7776069 ,  0.17766014,\n",
      "        0.784363  , -0.47345203, -0.5396144 ,  0.53295726,  1.2481446 ,\n",
      "       -1.6069899 ,  0.35911694,  0.7464602 , -0.7425208 , -1.8741494 ,\n",
      "        1.2066144 , -0.11411051,  1.3172673 , -2.5508647 ,  0.36404836,\n",
      "        0.6125904 ,  0.5474226 ,  0.8869986 , -1.0323466 ], dtype=float32), 'Lambda_im': Array([-2.08602266e+04, -6.95201465e+03, -4.16953467e+03, -2.97644360e+03,\n",
      "       -2.31315308e+03, -1.89068018e+03, -1.59787878e+03, -1.38288135e+03,\n",
      "       -1.21822815e+03, -1.08802209e+03, -9.82422241e+02, -8.95009827e+02,\n",
      "       -8.21421631e+02, -7.58585571e+02, -7.04278442e+02, -6.56849976e+02,\n",
      "       -6.15050598e+02, -5.77916077e+02, -5.44690735e+02, -5.14774048e+02,\n",
      "       -4.87682526e+02, -4.63022949e+02, -4.40471771e+02, -4.19760620e+02,\n",
      "       -4.00664978e+02, -3.82995636e+02, -3.66591766e+02, -3.51316254e+02,\n",
      "       -3.37050842e+02, -3.23693665e+02, -3.11155975e+02, -2.99360687e+02,\n",
      "       -2.88239746e+02, -2.77733643e+02, -2.67789459e+02, -2.58360352e+02,\n",
      "       -2.49404770e+02, -2.40885361e+02, -2.32768890e+02, -2.25025330e+02,\n",
      "       -2.17627563e+02, -2.10551392e+02, -2.03774460e+02, -1.97276917e+02,\n",
      "       -1.91040482e+02, -1.85048523e+02, -1.79285645e+02, -1.73738144e+02,\n",
      "       -1.68393143e+02, -1.63238770e+02, -1.58264481e+02, -1.53460129e+02,\n",
      "       -1.48816528e+02, -1.44325241e+02, -1.39978256e+02, -1.35768311e+02,\n",
      "       -1.31688568e+02, -1.27732727e+02, -1.23894905e+02, -1.20169746e+02,\n",
      "       -1.16551819e+02, -1.13036499e+02, -1.09619301e+02, -1.06296013e+02,\n",
      "       -1.03062721e+02, -9.99156342e+01, -9.68513260e+01, -9.38666306e+01,\n",
      "       -9.09583282e+01, -8.81235275e+01, -8.53596115e+01, -8.26639938e+01,\n",
      "       -8.00340500e+01, -7.74676895e+01, -7.49626999e+01, -7.25169754e+01,\n",
      "       -7.01286697e+01, -6.77959061e+01, -6.55168686e+01, -6.32900238e+01,\n",
      "       -6.11137047e+01, -5.89864311e+01, -5.69069214e+01, -5.48737106e+01,\n",
      "       -5.28855858e+01, -5.09413338e+01, -4.90397568e+01, -4.71799088e+01,\n",
      "       -4.53605614e+01, -4.35808945e+01, -4.18400650e+01, -4.01369591e+01,\n",
      "       -3.84708481e+01, -3.68410072e+01, -3.52465820e+01, -3.36869049e+01,\n",
      "       -3.21613731e+01, -3.06692600e+01, -2.92100811e+01, -2.77831306e+01,\n",
      "       -2.63880577e+01, -2.50242882e+01, -2.36914978e+01, -2.23890629e+01,\n",
      "       -2.11167698e+01, -1.98742237e+01, -1.86611614e+01, -1.74773998e+01,\n",
      "       -1.63226814e+01, -1.51965742e+01, -1.40992451e+01, -1.30305605e+01,\n",
      "       -1.19904242e+01, -1.09788876e+01, -9.99610615e+00, -9.04219532e+00,\n",
      "       -8.11740494e+00, -7.22227240e+00, -6.35736132e+00, -5.52320433e+00,\n",
      "       -4.72038269e+00, -3.95086884e+00, -3.21581388e+00, -2.51764107e+00,\n",
      "       -1.85971582e+00, -1.24746227e+00, -6.91304386e-01, -2.12408751e-01],      dtype=float32), 'Lambda_re': Array([-0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998, -0.4999998, -0.4999998,\n",
      "       -0.4999998, -0.4999998, -0.4999998], dtype=float32), 'log_step': Array([[-4.401313 ],\n",
      "       [-3.4231832],\n",
      "       [-6.455684 ],\n",
      "       [-5.591773 ],\n",
      "       [-4.7991114],\n",
      "       [-3.808304 ],\n",
      "       [-6.720236 ],\n",
      "       [-2.8873096],\n",
      "       [-5.1271553],\n",
      "       [-6.060017 ],\n",
      "       [-6.8918085],\n",
      "       [-5.9884887],\n",
      "       [-2.429522 ],\n",
      "       [-4.550123 ],\n",
      "       [-6.006946 ],\n",
      "       [-2.3750396],\n",
      "       [-4.995067 ],\n",
      "       [-6.883581 ],\n",
      "       [-4.155197 ],\n",
      "       [-3.9983556],\n",
      "       [-2.5248265],\n",
      "       [-6.0813894],\n",
      "       [-3.9428573],\n",
      "       [-6.6731367],\n",
      "       [-4.9277887],\n",
      "       [-6.4151435],\n",
      "       [-6.132242 ],\n",
      "       [-4.414331 ],\n",
      "       [-3.2828944],\n",
      "       [-4.4071627],\n",
      "       [-2.7928486],\n",
      "       [-3.924171 ],\n",
      "       [-3.2144377],\n",
      "       [-3.2280102],\n",
      "       [-5.13523  ],\n",
      "       [-2.4590893],\n",
      "       [-2.3289728],\n",
      "       [-2.3254862],\n",
      "       [-2.3675308],\n",
      "       [-3.608361 ],\n",
      "       [-3.7234445],\n",
      "       [-4.2330637],\n",
      "       [-4.3286085],\n",
      "       [-3.7794337],\n",
      "       [-6.8227315],\n",
      "       [-3.9205523],\n",
      "       [-5.9782586],\n",
      "       [-5.1130695],\n",
      "       [-4.2229147],\n",
      "       [-5.327427 ],\n",
      "       [-4.1695824],\n",
      "       [-3.1484845],\n",
      "       [-6.2256727],\n",
      "       [-4.7470126],\n",
      "       [-5.1046734],\n",
      "       [-5.197504 ],\n",
      "       [-4.6845417],\n",
      "       [-5.3839097],\n",
      "       [-3.7077932],\n",
      "       [-5.1441298],\n",
      "       [-3.2128282],\n",
      "       [-5.618268 ],\n",
      "       [-6.4176726],\n",
      "       [-6.5149226],\n",
      "       [-6.7652507],\n",
      "       [-6.6828165],\n",
      "       [-6.387664 ],\n",
      "       [-2.354148 ],\n",
      "       [-2.790801 ],\n",
      "       [-6.490747 ],\n",
      "       [-6.6131296],\n",
      "       [-3.036619 ],\n",
      "       [-3.6641273],\n",
      "       [-5.203851 ],\n",
      "       [-4.770872 ],\n",
      "       [-3.5033765],\n",
      "       [-3.5560784],\n",
      "       [-6.7345405],\n",
      "       [-4.7094927],\n",
      "       [-3.5685968],\n",
      "       [-6.612258 ],\n",
      "       [-6.7181587],\n",
      "       [-4.978254 ],\n",
      "       [-6.358854 ],\n",
      "       [-3.8391006],\n",
      "       [-3.653077 ],\n",
      "       [-3.194001 ],\n",
      "       [-2.3263407],\n",
      "       [-4.910762 ],\n",
      "       [-3.7298083],\n",
      "       [-2.5726252],\n",
      "       [-5.9630995],\n",
      "       [-4.8936243],\n",
      "       [-2.7328606],\n",
      "       [-2.8473177],\n",
      "       [-3.3449702],\n",
      "       [-2.7437391],\n",
      "       [-2.6739497],\n",
      "       [-3.7642837],\n",
      "       [-3.1505613],\n",
      "       [-5.9788804],\n",
      "       [-6.87231  ],\n",
      "       [-3.0724387],\n",
      "       [-4.583208 ],\n",
      "       [-5.2635374],\n",
      "       [-2.8262358],\n",
      "       [-6.8853993],\n",
      "       [-6.675846 ],\n",
      "       [-6.8788986],\n",
      "       [-5.8472085],\n",
      "       [-3.6133242],\n",
      "       [-5.971653 ],\n",
      "       [-3.7782292],\n",
      "       [-6.4506435],\n",
      "       [-5.8252583],\n",
      "       [-2.887261 ],\n",
      "       [-4.625204 ],\n",
      "       [-2.901816 ],\n",
      "       [-2.5920715],\n",
      "       [-3.8411505],\n",
      "       [-4.9044037],\n",
      "       [-5.4797473],\n",
      "       [-5.077378 ],\n",
      "       [-6.8155704],\n",
      "       [-5.7377777],\n",
      "       [-2.8249116],\n",
      "       [-2.4219732],\n",
      "       [-2.5396652]], dtype=float32)}}}}\n",
      "[*] Starting Training Epoch 1...\n",
      "using linear warmup for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3375 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'intermediates'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[6], line 236\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m    233\u001B[0m lr_params \u001B[38;5;241m=\u001B[39m (decay_function, ssm_lr, lr, step, end_step, args\u001B[38;5;241m.\u001B[39mopt_config, args\u001B[38;5;241m.\u001B[39mlr_min)\n\u001B[1;32m    235\u001B[0m train_rng, skey \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39msplit(train_rng)\n\u001B[0;32m--> 236\u001B[0m state, train_loss, step \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    237\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mskey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mmodel_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    239\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mtrainloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    240\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mseq_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    241\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43min_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    242\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatchnorm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    243\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mlr_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m valloader \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[*] Running Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Validation...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[6], line 55\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(state, rng, model, trainloader, seq_len, in_dim, batchnorm, lr_params)\u001B[0m\n\u001B[1;32m     53\u001B[0m inputs, labels, integration_times \u001B[38;5;241m=\u001B[39m prep_batch(batch, seq_len, in_dim)\n\u001B[1;32m     54\u001B[0m rng, drop_rng \u001B[38;5;241m=\u001B[39m jax\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39msplit(rng)\n\u001B[0;32m---> 55\u001B[0m state, loss \u001B[38;5;241m=\u001B[39m \u001B[43mhidden_gradient_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdrop_rng\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[43m    \u001B[49m\u001B[43mintegration_times\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     61\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     62\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatchnorm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m batch_losses\u001B[38;5;241m.\u001B[39mappend(loss)\n\u001B[1;32m     65\u001B[0m lr_params \u001B[38;5;241m=\u001B[39m (decay_function, ssm_lr, lr, step, end_step, opt_config, lr_min)\n",
      "    \u001B[0;31m[... skipping hidden 11 frame]\u001B[0m\n",
      "Cell \u001B[0;32mIn[6], line 34\u001B[0m, in \u001B[0;36mhidden_gradient_step\u001B[0;34m(state, rng, batch_inputs, batch_labels, batch_integration_timesteps, model, batchnorm)\u001B[0m\n\u001B[1;32m     30\u001B[0m     loss \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(cross_entropy_loss(logits, batch_labels))\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss, (mod_vars, logits)\n\u001B[0;32m---> 34\u001B[0m (loss, (mod_vars, logits)), grads \u001B[38;5;241m=\u001B[39m \u001B[43mjax\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue_and_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhas_aux\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batchnorm:\n\u001B[1;32m     37\u001B[0m     state \u001B[38;5;241m=\u001B[39m state\u001B[38;5;241m.\u001B[39mapply_gradients(grads\u001B[38;5;241m=\u001B[39mgrads, batch_stats\u001B[38;5;241m=\u001B[39mmod_vars[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_stats\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "    \u001B[0;31m[... skipping hidden 8 frame]\u001B[0m\n",
      "Cell \u001B[0;32mIn[6], line 21\u001B[0m, in \u001B[0;36mhidden_gradient_step.<locals>.loss_fn\u001B[0;34m(params)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batchnorm:\n\u001B[1;32m     14\u001B[0m     logits, mod_vars \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mapply(\n\u001B[1;32m     15\u001B[0m         {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m\"\u001B[39m: params, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_stats\u001B[39m\u001B[38;5;124m\"\u001B[39m: state\u001B[38;5;241m.\u001B[39mbatch_stats},\n\u001B[1;32m     16\u001B[0m         batch_inputs, batch_integration_timesteps,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     19\u001B[0m         capture_intermediates\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     20\u001B[0m     )\n\u001B[0;32m---> 21\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[43mmod_vars\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mintermediates\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     23\u001B[0m     logits, mod_vars \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mapply(\n\u001B[1;32m     24\u001B[0m         {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m\"\u001B[39m: params},\n\u001B[1;32m     25\u001B[0m         batch_inputs, batch_integration_timesteps,\n\u001B[1;32m     26\u001B[0m         rngs\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdropout\u001B[39m\u001B[38;5;124m\"\u001B[39m: rng},\n\u001B[1;32m     27\u001B[0m         mutable\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mintermediates\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m     28\u001B[0m     )\n",
      "\u001B[0;31mKeyError\u001B[0m: 'intermediates'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T02:56:43.905965Z",
     "start_time": "2025-01-15T02:56:43.905667Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "911b19a641ca63d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6817e7f7a057c0a2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

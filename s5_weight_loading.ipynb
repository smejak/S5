{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-09T17:23:44.802845Z",
     "start_time": "2025-01-09T17:23:44.800340Z"
    }
   },
   "source": [
    "import pickle\n",
    "from functools import partial\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as np\n",
    "from s5.seq_model import BatchClassificationModel, RetrievalModel\n",
    "from s5.ssm import init_S5SSM\n",
    "from s5.ssm_init import make_DPLR_HiPPO\n",
    "from s5.dataloading import Datasets\n",
    "from s5.train_helpers import validate, create_train_state, map_nested_fn\n",
    "from jax.scipy.linalg import block_diag\n",
    "\n",
    "# Function to load weights from pickle file\n",
    "def load_weights(pickle_file):\n",
    "    with open(pickle_file, \"rb\") as f:\n",
    "        params = pickle.load(f)\n",
    "    return params"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T17:22:38.628398Z",
     "start_time": "2025-01-09T17:22:38.621856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dclass Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "# Create the args object\n",
    "args = Args(\n",
    "    ssm_size=256,\n",
    "    jax_seed=0,\n",
    "    blocks=4,\n",
    "    d_model=128,\n",
    "    n_layers=6,\n",
    "    n_classes=2,\n",
    "    C_init='complex_normal',\n",
    "    discretization='zoh',\n",
    "    dt_min=0.001,\n",
    "    dt_max=0.1,\n",
    "    conj_sym=True,\n",
    "    clip_eigs=True,\n",
    "    bidirectional=False,\n",
    "    activation_fn='gelu',\n",
    "    dropout=0.0,\n",
    "    prenorm=True,\n",
    "    batchnorm=True,\n",
    "    bn_momentum=0.9,\n",
    "    dir_name='./cache_dir',\n",
    "    bsz=64,\n",
    "    ssm_size_base=256,\n",
    "    p_dropout=0.0,\n",
    "    mode='pool',\n",
    "    lr_factor=1,\n",
    "    ssm_lr_base=1e-3,\n",
    "    weight_decay=0.05,\n",
    "    opt_config='standard',\n",
    "    dt_global=False,\n",
    "    dataset='mnist-classification',\n",
    "    epochs=1,\n",
    "    warmup_end=1,\n",
    "    lr_min=0,\n",
    ")\n",
    "\n",
    "pickle_file = \"quick_test.pkl\"\n",
    "params = load_weights(pickle_file)"
   ],
   "id": "71d39ae03d38b8ad",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T17:22:10.095774Z",
     "start_time": "2025-01-09T17:22:01.184385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ssm_size = args.ssm_size_base\n",
    "ssm_lr = args.ssm_lr_base\n",
    "\n",
    "# determine the size of initial blocks\n",
    "block_size = int(ssm_size / args.blocks)\n",
    "\n",
    "# Set global learning rate lr (e.g. encoders, etc.) as function of ssm_lr\n",
    "lr = args.lr_factor * ssm_lr\n",
    "\n",
    "# Set randomness...\n",
    "print(\"[*] Setting Randomness...\")\n",
    "key = random.PRNGKey(args.jax_seed)\n",
    "init_rng, train_rng = random.split(key, num=2)\n",
    "\n",
    "# Get dataset creation function\n",
    "create_dataset_fn = Datasets[args.dataset]\n",
    "\n",
    "# Dataset dependent logic\n",
    "if args.dataset in [\"imdb-classification\", \"listops-classification\", \"aan-classification\"]:\n",
    "    padded = True\n",
    "    if args.dataset in [\"aan-classification\"]:\n",
    "        # Use retreival model for document matching\n",
    "        retrieval = True\n",
    "        print(\"Using retrieval model for document matching\")\n",
    "    else:\n",
    "        retrieval = False\n",
    "\n",
    "else:\n",
    "    padded = False\n",
    "    retrieval = False\n",
    "\n",
    "\n",
    "init_rng, key = random.split(init_rng, num=2)\n",
    "trainloader, valloader, testloader, aux_dataloaders, n_classes, seq_len, in_dim, train_size = \\\n",
    "  create_dataset_fn(args.dir_name, seed=args.jax_seed, bsz=args.bsz)\n",
    "\n",
    "print(f\"[*] Starting S5 Training on `{args.dataset}` =>> Initializing...\")\n",
    "\n",
    "# Initialize state matrix A using approximation to HiPPO-LegS matrix\n",
    "Lambda, _, B, V, B_orig = make_DPLR_HiPPO(block_size)\n",
    "\n",
    "if args.conj_sym:\n",
    "    block_size = block_size // 2\n",
    "    ssm_size = ssm_size // 2\n",
    "\n",
    "Lambda = Lambda[:block_size]\n",
    "V = V[:, :block_size]\n",
    "Vc = V.conj().T\n",
    "\n",
    "# If initializing state matrix A as block-diagonal, put HiPPO approximation\n",
    "# on each block\n",
    "Lambda = (Lambda * np.ones((args.blocks, block_size))).ravel()\n",
    "V = block_diag(*([V] * args.blocks))\n",
    "Vinv = block_diag(*([Vc] * args.blocks))\n",
    "\n",
    "print(\"Lambda.shape={}\".format(Lambda.shape))\n",
    "print(\"V.shape={}\".format(V.shape))\n",
    "print(\"Vinv.shape={}\".format(Vinv.shape))\n",
    "\n",
    "ssm_init_fn = init_S5SSM(H=args.d_model,\n",
    "                         P=ssm_size,\n",
    "                         Lambda_re_init=Lambda.real,\n",
    "                         Lambda_im_init=Lambda.imag,\n",
    "                         V=V,\n",
    "                         Vinv=Vinv,\n",
    "                         C_init=args.C_init,\n",
    "                         discretization=args.discretization,\n",
    "                         dt_min=args.dt_min,\n",
    "                         dt_max=args.dt_max,\n",
    "                         conj_sym=args.conj_sym,\n",
    "                         clip_eigs=args.clip_eigs,\n",
    "                         bidirectional=args.bidirectional)\n",
    "if retrieval:\n",
    "    # Use retrieval head for AAN task\n",
    "    print(\"Using Retrieval head for {} task\".format(args.dataset))\n",
    "    model_cls = partial(\n",
    "        RetrievalModel,\n",
    "        ssm=ssm_init_fn,\n",
    "        d_output=n_classes,\n",
    "        d_model=args.d_model,\n",
    "        n_layers=args.n_layers,\n",
    "        padded=padded,\n",
    "        activation=args.activation_fn,\n",
    "        dropout=args.p_dropout,\n",
    "        prenorm=args.prenorm,\n",
    "        batchnorm=args.batchnorm,\n",
    "        bn_momentum=args.bn_momentum,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    model_cls = partial(\n",
    "        BatchClassificationModel,\n",
    "        ssm=ssm_init_fn,\n",
    "        d_output=n_classes,\n",
    "        d_model=args.d_model,\n",
    "        n_layers=args.n_layers,\n",
    "        padded=padded,\n",
    "        activation=args.activation_fn,\n",
    "        dropout=args.p_dropout,\n",
    "        mode=args.mode,\n",
    "        prenorm=args.prenorm,\n",
    "        batchnorm=args.batchnorm,\n",
    "        bn_momentum=args.bn_momentum,\n",
    "    )\n",
    "\n",
    "# initialize training state\n",
    "state = create_train_state(model_cls,\n",
    "                           init_rng,\n",
    "                           padded,\n",
    "                           retrieval,\n",
    "                           in_dim=in_dim,\n",
    "                           bsz=args.bsz,\n",
    "                           seq_len=seq_len,\n",
    "                           weight_decay=args.weight_decay,\n",
    "                           batchnorm=args.batchnorm,\n",
    "                           opt_config=args.opt_config,\n",
    "                           ssm_lr=ssm_lr,\n",
    "                           lr=lr,\n",
    "                           dt_global=args.dt_global)\n",
    "\n",
    "# Training Loop over epochs\n",
    "best_loss, best_acc, best_epoch = 100000000, -100000000.0, 0  # This best loss is val_loss\n",
    "count, best_val_loss = 0, 100000000  # This line is for early stopping purposes\n",
    "lr_count, opt_acc = 0, -100000000.0  # This line is for learning rate decay\n",
    "step = 0  # for per step learning rate decay\n",
    "steps_per_epoch = int(train_size/args.bsz)\n"
   ],
   "id": "a5d0b3ee0d4e76e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Setting Randomness...\n",
      "[*] Generating MNIST Classification Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakub/miniconda3/envs/jax-env/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/jakub/miniconda3/envs/jax-env/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <9DBE5D5C-AC87-30CA-96DA-F5BC116EDA2B> /Users/jakub/miniconda3/envs/jax-env/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <D400622C-0C6B-3AE1-AB45-F1D0BF19B384> /Users/jakub/miniconda3/envs/jax-env/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Starting S5 Training on `mnist-classification` =>> Initializing...\n",
      "Lambda.shape=(128,)\n",
      "V.shape=(256, 128)\n",
      "Vinv.shape=(128, 256)\n",
      "configuring standard optimization setup\n",
      "[*] Trainable Parameters: 399370\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T17:25:26.238662Z",
     "start_time": "2025-01-09T17:24:32.036682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from flax.training.train_state import TrainState\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "from typing import Any\n",
    "\n",
    "def create_test_state(model_cls,\n",
    "                      rng,\n",
    "                      params,\n",
    "                      padded,\n",
    "                      retrieval,\n",
    "                      in_dim=1,\n",
    "                      bsz=128,\n",
    "                      seq_len=784,\n",
    "                      dt_global=False):\n",
    "    \"\"\"\n",
    "    Initializes the test state by loading provided parameters.\n",
    "\n",
    "    :param model_cls:\n",
    "    :param rng:\n",
    "    :param params: The weights to be loaded in.\n",
    "    :param padded:\n",
    "    :param retrieval:\n",
    "    :param in_dim:\n",
    "    :param bsz:\n",
    "    :param seq_len:\n",
    "    :param dt_global:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if padded:\n",
    "        if retrieval:\n",
    "            # For retrieval tasks we have two different sets of \"documents\"\n",
    "            dummy_input = (np.ones((2 * bsz, seq_len, in_dim)), np.ones(2 * bsz))\n",
    "            integration_timesteps = np.ones((2 * bsz, seq_len,))\n",
    "        else:\n",
    "            dummy_input = (np.ones((bsz, seq_len, in_dim)), np.ones(bsz))\n",
    "            integration_timesteps = np.ones((bsz, seq_len,))\n",
    "    else:\n",
    "        dummy_input = np.ones((bsz, seq_len, in_dim))\n",
    "        integration_timesteps = np.ones((bsz, seq_len, ))\n",
    "\n",
    "    model = model_cls(training=False)\n",
    "    init_rng, dropout_rng = jax.random.split(rng, num=2)\n",
    "    variables = model.init({\"params\": init_rng, \"dropout\": dropout_rng},\n",
    "               dummy_input, integration_timesteps)\n",
    "\n",
    "    fn_is_complex = lambda x: x.dtype in [np.complex64, np.complex128]\n",
    "    param_sizes = map_nested_fn(lambda k, param: param.size * (2 if fn_is_complex(param) else 1))(params)\n",
    "    print(f\"[*] Loaded Parameters: {sum(jax.tree.leaves(param_sizes))}\")\n",
    "\n",
    "    \"\"\"This option applies weight decay to C, but B is kept with the\n",
    "        SSM parameters with no weight decay.\n",
    "    \"\"\"\n",
    "    print(\"configuring standard optimization setup\")\n",
    "    if dt_global:\n",
    "        ssm_fn = map_nested_fn(\n",
    "            lambda k, _: \"ssm\"\n",
    "            if k in [\"B\", \"Lambda_re\", \"Lambda_im\", \"norm\"]\n",
    "            else (\"none\" if k in [] else \"regular\")\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        ssm_fn = map_nested_fn(\n",
    "            lambda k, _: \"ssm\"\n",
    "            if k in [\"B\", \"Lambda_re\", \"Lambda_im\", \"log_step\", \"norm\"]\n",
    "            else (\"none\" if k in [] else \"regular\")\n",
    "        )\n",
    "    tx = optax.multi_transform(\n",
    "        {\n",
    "            \"none\": optax.inject_hyperparams(optax.sgd)(learning_rate=0.0),\n",
    "            \"ssm\": optax.inject_hyperparams(optax.adam)(learning_rate=ssm_lr),\n",
    "            \"regular\": optax.inject_hyperparams(optax.adamw)(learning_rate=lr,\n",
    "                                                             weight_decay=0.01),\n",
    "        },\n",
    "        ssm_fn,\n",
    "    )\n",
    "    batch_stats = variables['batch_stats']\n",
    "    class TrainState(train_state.TrainState):\n",
    "        batch_stats: Any\n",
    "    return TrainState.create(apply_fn=model.apply, params=params, tx=tx, batch_stats=batch_stats)\n",
    "\n",
    "test_state = create_test_state(model_cls,\n",
    "                               init_rng,\n",
    "                               params,\n",
    "                               padded,\n",
    "                               retrieval)\n",
    "\n",
    "val_loss, val_acc = validate(test_state,\n",
    "                             model_cls,\n",
    "                             valloader,\n",
    "                             seq_len,\n",
    "                             in_dim,\n",
    "                             args.batchnorm)"
   ],
   "id": "309dead207651a5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loaded Parameters: 399370\n",
      "configuring standard optimization setup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:51<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T17:25:28.984583Z",
     "start_time": "2025-01-09T17:25:28.979903Z"
    }
   },
   "cell_type": "code",
   "source": "val_loss, val_acc",
   "id": "d93c1d7085d3d85b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(246.7057, dtype=float32), Array(0.08633333, dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T17:25:48.164820Z",
     "start_time": "2025-01-09T17:25:48.162749Z"
    }
   },
   "cell_type": "code",
   "source": "args.batchnorm",
   "id": "f23040d50979e561",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e396648e552c706e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

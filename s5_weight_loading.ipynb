{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-09T17:54:09.058845Z",
     "start_time": "2025-01-09T17:54:09.045658Z"
    }
   },
   "source": [
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "import jax.numpy as np\n",
    "from jax import random\n",
    "from jax.scipy.linalg import block_diag\n",
    "\n",
    "from s5.dataloading import Datasets\n",
    "from s5.seq_model import BatchClassificationModel, RetrievalModel\n",
    "from s5.ssm import init_S5SSM\n",
    "from s5.ssm_init import make_DPLR_HiPPO\n",
    "from s5.train_helpers import create_train_state, reduce_lr_on_plateau, \\\n",
    "    linear_warmup, cosine_annealing, constant_lr, train_epoch, validate\n",
    "\n",
    "import pickle\n",
    "from functools import partial\n",
    "from typing import Any\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "from jax import random\n",
    "from jax.scipy.linalg import block_diag\n",
    "\n",
    "from s5.dataloading import Datasets\n",
    "from s5.seq_model import BatchClassificationModel, RetrievalModel\n",
    "from s5.ssm import init_S5SSM\n",
    "from s5.ssm_init import make_DPLR_HiPPO\n",
    "from s5.train_helpers import map_nested_fn, validate\n",
    "\n",
    "\n",
    "def load_weights(pickle_file):\n",
    "    with open(pickle_file, \"rb\") as f:\n",
    "        params = pickle.load(f)\n",
    "    return params\n",
    "\n",
    "\n",
    "def create_test_state(model_cls,\n",
    "                      rng,\n",
    "                      params,\n",
    "                      padded,\n",
    "                      retrieval,\n",
    "                      in_dim=1,\n",
    "                      bsz=128,\n",
    "                      seq_len=784,\n",
    "                      ssm_lr=0,\n",
    "                      lr=0,\n",
    "                      dt_global=False):\n",
    "    \"\"\"\n",
    "    Initializes the test state by loading provided parameters.\n",
    "\n",
    "    :param model_cls:\n",
    "    :param rng:\n",
    "    :param params: The weights to be loaded in.\n",
    "    :param padded:\n",
    "    :param retrieval:\n",
    "    :param in_dim:\n",
    "    :param bsz:\n",
    "    :param seq_len:\n",
    "    :param dt_global:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if padded:\n",
    "        if retrieval:\n",
    "            # For retrieval tasks we have two different sets of \"documents\"\n",
    "            dummy_input = (np.ones((2 * bsz, seq_len, in_dim)), np.ones(2 * bsz))\n",
    "            integration_timesteps = np.ones((2 * bsz, seq_len,))\n",
    "        else:\n",
    "            dummy_input = (np.ones((bsz, seq_len, in_dim)), np.ones(bsz))\n",
    "            integration_timesteps = np.ones((bsz, seq_len,))\n",
    "    else:\n",
    "        dummy_input = np.ones((bsz, seq_len, in_dim))\n",
    "        integration_timesteps = np.ones((bsz, seq_len,))\n",
    "\n",
    "    model = model_cls(training=False)\n",
    "    init_rng, dropout_rng = jax.random.split(rng, num=2)\n",
    "    variables = model.init({\"params\": init_rng, \"dropout\": dropout_rng},\n",
    "                           dummy_input, integration_timesteps)\n",
    "\n",
    "    fn_is_complex = lambda x: x.dtype in [np.complex64, np.complex128]\n",
    "    param_sizes = map_nested_fn(lambda k, param: param.size * (2 if fn_is_complex(param) else 1))(params)\n",
    "    print(f\"[*] Loaded Parameters: {sum(jax.tree.leaves(param_sizes))}\")\n",
    "\n",
    "    \"\"\"This option applies weight decay to C, but B is kept with the\n",
    "        SSM parameters with no weight decay.\n",
    "    \"\"\"\n",
    "    print(\"configuring standard optimization setup\")\n",
    "    if dt_global:\n",
    "        ssm_fn = map_nested_fn(\n",
    "            lambda k, _: \"ssm\"\n",
    "            if k in [\"B\", \"Lambda_re\", \"Lambda_im\", \"norm\"]\n",
    "            else (\"none\" if k in [] else \"regular\")\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        ssm_fn = map_nested_fn(\n",
    "            lambda k, _: \"ssm\"\n",
    "            if k in [\"B\", \"Lambda_re\", \"Lambda_im\", \"log_step\", \"norm\"]\n",
    "            else (\"none\" if k in [] else \"regular\")\n",
    "        )\n",
    "    tx = optax.multi_transform(\n",
    "        {\n",
    "            \"none\": optax.inject_hyperparams(optax.sgd)(learning_rate=0.0),\n",
    "            \"ssm\": optax.inject_hyperparams(optax.adam)(learning_rate=ssm_lr),\n",
    "            \"regular\": optax.inject_hyperparams(optax.adamw)(learning_rate=lr,\n",
    "                                                             weight_decay=0.01),\n",
    "        },\n",
    "        ssm_fn,\n",
    "    )\n",
    "    print(variables.keys())\n",
    "    # batch_stats = variables['batch_stats']\n",
    "\n",
    "    # class TrainState(train_state.TrainState):\n",
    "    #     batch_stats: Any\n",
    "\n",
    "    # return TrainState.create(apply_fn=model.apply, params=params, tx=tx, batch_stats=batch_stats)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "def test(args):\n",
    "    # load dataset\n",
    "    ssm_size = args.ssm_size_base\n",
    "    ssm_lr = args.ssm_lr_base\n",
    "\n",
    "    # determine the size of initial blocks\n",
    "    block_size = int(ssm_size / args.blocks)\n",
    "\n",
    "    # Set global learning rate lr (e.g. encoders, etc.) as function of ssm_lr\n",
    "    lr = args.lr_factor * ssm_lr\n",
    "\n",
    "    # Set randomness...\n",
    "    print(\"[*] Setting Randomness...\")\n",
    "    key = random.PRNGKey(args.jax_seed)\n",
    "    init_rng, train_rng = random.split(key, num=2)\n",
    "\n",
    "    # Get dataset creation function\n",
    "    create_dataset_fn = Datasets[args.dataset]\n",
    "\n",
    "    # Dataset dependent logic\n",
    "    if args.dataset in [\"imdb-classification\", \"listops-classification\", \"aan-classification\"]:\n",
    "        padded = True\n",
    "        if args.dataset in [\"aan-classification\"]:\n",
    "            # Use retreival model for document matching\n",
    "            retrieval = True\n",
    "            print(\"Using retrieval model for document matching\")\n",
    "        else:\n",
    "            retrieval = False\n",
    "\n",
    "    else:\n",
    "        padded = False\n",
    "        retrieval = False\n",
    "\n",
    "    init_rng, key = random.split(init_rng, num=2)\n",
    "    trainloader, valloader, testloader, aux_dataloaders, n_classes, seq_len, in_dim, train_size = \\\n",
    "        create_dataset_fn(args.dir_name, seed=args.jax_seed, bsz=args.bsz)\n",
    "\n",
    "    print(f\"[*] Starting S5 Testing on `{args.dataset}` =>> Initializing...\")\n",
    "\n",
    "    # Initialize state matrix A using approximation to HiPPO-LegS matrix\n",
    "    Lambda, _, B, V, B_orig = make_DPLR_HiPPO(block_size)\n",
    "\n",
    "    if args.conj_sym:\n",
    "        block_size = block_size // 2\n",
    "        ssm_size = ssm_size // 2\n",
    "\n",
    "    Lambda = Lambda[:block_size]\n",
    "    V = V[:, :block_size]\n",
    "    Vc = V.conj().T\n",
    "\n",
    "    # If initializing state matrix A as block-diagonal, put HiPPO approximation\n",
    "    # on each block\n",
    "    Lambda = (Lambda * np.ones((args.blocks, block_size))).ravel()\n",
    "    V = block_diag(*([V] * args.blocks))\n",
    "    Vinv = block_diag(*([Vc] * args.blocks))\n",
    "\n",
    "    print(\"Lambda.shape={}\".format(Lambda.shape))\n",
    "    print(\"V.shape={}\".format(V.shape))\n",
    "    print(\"Vinv.shape={}\".format(Vinv.shape))\n",
    "\n",
    "    ssm_init_fn = init_S5SSM(H=args.d_model,\n",
    "                             P=ssm_size,\n",
    "                             Lambda_re_init=Lambda.real,\n",
    "                             Lambda_im_init=Lambda.imag,\n",
    "                             V=V,\n",
    "                             Vinv=Vinv,\n",
    "                             C_init=args.C_init,\n",
    "                             discretization=args.discretization,\n",
    "                             dt_min=args.dt_min,\n",
    "                             dt_max=args.dt_max,\n",
    "                             conj_sym=args.conj_sym,\n",
    "                             clip_eigs=args.clip_eigs,\n",
    "                             bidirectional=args.bidirectional)\n",
    "    if retrieval:\n",
    "        # Use retrieval head for AAN task\n",
    "        print(\"Using Retrieval head for {} task\".format(args.dataset))\n",
    "        model_cls = partial(\n",
    "            RetrievalModel,\n",
    "            ssm=ssm_init_fn,\n",
    "            d_output=n_classes,\n",
    "            d_model=args.d_model,\n",
    "            n_layers=args.n_layers,\n",
    "            padded=padded,\n",
    "            activation=args.activation_fn,\n",
    "            dropout=args.p_dropout,\n",
    "            prenorm=args.prenorm,\n",
    "            batchnorm=args.batchnorm,\n",
    "            bn_momentum=args.bn_momentum,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        model_cls = partial(\n",
    "            BatchClassificationModel,\n",
    "            ssm=ssm_init_fn,\n",
    "            d_output=n_classes,\n",
    "            d_model=args.d_model,\n",
    "            n_layers=args.n_layers,\n",
    "            padded=padded,\n",
    "            activation=args.activation_fn,\n",
    "            dropout=args.p_dropout,\n",
    "            mode=args.mode,\n",
    "            prenorm=args.prenorm,\n",
    "            batchnorm=args.batchnorm,\n",
    "            bn_momentum=args.bn_momentum,\n",
    "        )\n",
    "\n",
    "    # load weights\n",
    "    params = load_weights(args.weights_path)\n",
    "\n",
    "    # initialize test state\n",
    "    state = create_test_state(model_cls,\n",
    "                              init_rng,\n",
    "                              params,\n",
    "                              padded,\n",
    "                              retrieval,\n",
    "                              in_dim=in_dim,\n",
    "                              bsz=args.bsz,\n",
    "                              seq_len=seq_len,\n",
    "                              # weight_decay=args.weight_decay,\n",
    "                              # batchnorm=args.batchnorm,\n",
    "                              # opt_config=args.opt_config,\n",
    "                              ssm_lr=ssm_lr,\n",
    "                              lr=lr,\n",
    "                              dt_global=args.dt_global)\n",
    "\n",
    "    val_loss, val_acc = validate(state,\n",
    "                                 model_cls,\n",
    "                                 valloader,\n",
    "                                 seq_len,\n",
    "                                 in_dim,\n",
    "                                 args.batchnorm)\n",
    "\n",
    "    print(f\"[*] Validation Loss: {val_loss}, Validation Accuracy: {val_acc}\")\n",
    "    return val_loss, val_acc\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "\n",
    "# Create the args object\n",
    "args = Args(\n",
    "    ssm_size=256,\n",
    "    jax_seed=0,\n",
    "    blocks=4,\n",
    "    d_model=128,\n",
    "    n_layers=6,\n",
    "    n_classes=2,\n",
    "    C_init='complex_normal',\n",
    "    discretization='zoh',\n",
    "    dt_min=0.001,\n",
    "    dt_max=0.1,\n",
    "    conj_sym=True,\n",
    "    clip_eigs=True,\n",
    "    bidirectional=False,\n",
    "    activation_fn='gelu',\n",
    "    dropout=0.0,\n",
    "    prenorm=True,\n",
    "    batchnorm=False,\n",
    "    bn_momentum=0.9,\n",
    "    dir_name='./cache_dir',\n",
    "    bsz=64,\n",
    "    ssm_size_base=256,\n",
    "    p_dropout=0.0,\n",
    "    mode='pool',\n",
    "    lr_factor=1,\n",
    "    ssm_lr_base=1e-3,\n",
    "    weight_decay=0.05,\n",
    "    opt_config='standard',\n",
    "    dt_global=False,\n",
    "    dataset='mnist-classification',\n",
    "    epochs=1,\n",
    "    warmup_end=1,\n",
    "    lr_min=0,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T17:55:15.139169Z",
     "start_time": "2025-01-09T17:54:17.099147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "args.weights_path = \"./quick_test.pkl\"\n",
    "test(args)"
   ],
   "id": "fb53cc889d81921b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Setting Randomness...\n",
      "[*] Generating MNIST Classification Dataset\n",
      "[*] Starting S5 Testing on `mnist-classification` =>> Initializing...\n",
      "Lambda.shape=(128,)\n",
      "V.shape=(256, 128)\n",
      "Vinv.shape=(128, 256)\n",
      "[*] Loaded Parameters: 399370\n",
      "configuring standard optimization setup\n",
      "dict_keys(['params'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:55<00:00,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Validation Loss: 0.2723929286003113, Validation Accuracy: 0.9141666293144226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Array(0.27239293, dtype=float32), Array(0.9141666, dtype=float32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e396648e552c706e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
